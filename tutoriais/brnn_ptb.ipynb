{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brnn-ptb.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "mW0FRTDqJGX8",
        "e1SoYhT4JLc1",
        "tE8xzRR3VswX",
        "iqLkUvRJDrM9",
        "t5zaBZRG2Scx",
        "1drO8DKL4Arw",
        "jCfbN_DhDIUA",
        "RKViEXoIo3lB",
        "dyc4m1WSMWX8",
        "hub1P9OoyQZu",
        "1bQY1H7nHVX8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcvDY5hfeC8x",
        "colab_type": "text"
      },
      "source": [
        "# **Modelos Gráficos Probabilísticos - Trabalho Prático**\n",
        "\n",
        "**Universidade Federal de Minas Gerais | Programa de Pós-Graduação em Ciência da Computação**\n",
        "\n",
        "**Nome:** Leandro Augusto Lacerda Campos\n",
        "\n",
        "Dezembro de 2019\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d5mFP8l2cP3",
        "colab_type": "text"
      },
      "source": [
        "## **1. Introdução**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3PRskG4DZe9",
        "colab_type": "text"
      },
      "source": [
        "O presente trabalho prático (TP) consiste em implementar, utilizando a linguagem de programação Python 3 e bibliotecas do *framework* Apache MXNet, a abordagem bayesiana para redes neurais recorrentes (RNNs) sugerida por Fortunato et al. [2017]. Nessa abordagem, os autores adicionam incerteza e regularização a RNNs aplicando o método *bayes by backprop* (BBB) formulado por Blundell et al. [2015].\n",
        "\n",
        "Implementararemos, especificamente, a aplicação da abordagem proposta no problema de modelagem de linguagem. Nessa aplicação, Fortunato et al. [2017] utilizam o conjunto de dados de Marcus et al. [1993], denominado *Penn\n",
        "Treebank* (PTB), e a arquitetura de rede indicada por Zaremba et al. [2014] para desenvolver uma RNN bayesiana que tem por objetivo predizer a próxima palavra de uma sequência.\n",
        "\n",
        "Por fim, cabe ressaltar que não fazem parte do escopo desse TP:\n",
        "*   a implementação do método de adaptação local da aproximação variacional a lotes de dados, denominado *posterior sharpening* e proposto por Fortunato et al. [2017];\n",
        "*   a implementação do método de inferência dinâmica proposto por Mikolov et al. [2010]; e\n",
        "*   a aplicação da abordagem proposta por Fortunato et al. [2017] para a tarefa de geração de legenda para\n",
        "imagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW0FRTDqJGX8",
        "colab_type": "text"
      },
      "source": [
        "### **1.1. Suposições sobre o leitor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtM81dtPJKl9",
        "colab_type": "text"
      },
      "source": [
        "Assumimos que você, leitor, conhece os tópicos listados abaixo. Caso precise relembrá-los, incluímos algumas referências úteis:\n",
        "*   Redes neurais: preparação dos dados, modelagem e treinamento. Consulte os capítulos [2](http://d2l.ai/chapter_preliminaries/index.html), [3](http://d2l.ai/chapter_linear-networks/index.html) e [4](http://d2l.ai/chapter_multilayer-perceptrons/index.html) do livro de Zhang et al. [2019].\n",
        "*   Redes neurais recorrentes: preparação dos dados, arquitetura *long short-term memory* (LSTM), *gradient clipping* e a versão truncada do *backpropgation through time* (BPTT). Consulte os capítulos [8](http://d2l.ai/chapter_recurrent-neural-networks/index.html) e [9](http://d2l.ai/chapter_modern_recurrent-networks/index.html) do livro de Zhang et al. [2019].\n",
        "*   Processamento de linguagem natural: *word embedding*. Consulte as quatro primeiras seções do capítulo [14](http://d2l.ai/chapter_natural-language-processing/index.html) do livro de Zhang et al. [2019].\n",
        "*   Teoria da informação: entropia cruzada, perplexidade e divergência de Kullback-Leibler. Consulte as seções [17.11](http://d2l.ai/chapter_appendix_math/information-theory.html) e [8.4.4](http://d2l.ai/chapter_recurrent-neural-networks/rnn.html#perplexity) do livro de Zhang et al. [2019].\n",
        "*   Inferência bayesiana. Consulte a seguinte [página](https://en.wikipedia.org/wiki/Bayesian_inference) do Wikipedia.\n",
        "\n",
        "Também supomos que você tem experiência com o Apache MXNet (ou o PyTorch, que é similar) e Python 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1SoYhT4JLc1",
        "colab_type": "text"
      },
      "source": [
        "### **1.2. Suposições sobre o ambiente de execução**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhO4qBDcJQAg",
        "colab_type": "text"
      },
      "source": [
        "Esse TP foi planejado para ser executado em um ambiente do [Google Colab](https://colab.research.google.com/) com suporte a Python 3 e a unidade de processamento gráfico (GPU). Para verificar ou alterar as configurações desse notebook no Colab, clique no menu 'Edit' e depois em 'Notebook settings'. Vamos assumir que essa GPU é compatível com a versão 10.1 da NVIDIA CUDA e que esse *toolkit* já está instalado no ambiente.\n",
        "\n",
        "O tempo de execução desse TP nesse ambiente é de aproximadamente 02h30min. Esse tempo pode variar dependendo das configurações do ambiente no qual você está conectado. Nas melhores configurações já testadas, o treinamento de cada época do modelo de Zaremba et al. [2014] demora cerca de 35 segundos. Caso você experimente um tempo 2x ou 3x maior do que esse, sugerimos que você se desconecte do atual ambiente de execução (clique no menu 'Runtime' e depois em 'Manage sessions') e então se conecte novamente.\n",
        "\n",
        "Durante a execução desse notebook, você pode se deparar com os seguintes problemas:\n",
        "\n",
        "*   Ocorreu um erro de processamento ou de memória na GPU. Nesse caso, recomendamos que você se desconecte do atual ambiente de execução (clique no menu 'Runtime' e depois em 'Manage sessions') e então se conecte novamente.\n",
        "*   A conexão com o ambiente de execução foi interrompida por inatividade ou limite de tempo de uso excedido. Nessa situação, tente se conectar novamente clicando no botão 'Connect', situado no canto superior direito da página. As vezes, a execução recomeça de onde parou. Caso contrário, comece tudo novamente.\n",
        "\n",
        "É importante ressaltar que você não precisa executar esse notebook para avaliar os resultados do TP, uma vez que a saída de cada célula está salva na versão publicada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE8xzRR3VswX",
        "colab_type": "text"
      },
      "source": [
        "#### **1.2.1. Problemas com a GPU do Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NQfvbviV1gd",
        "colab_type": "text"
      },
      "source": [
        "Com uma certa frequência, a GPU do ambiente de execução do Colab ao qual você se conecta não consegue executar corretamente esse notebook. Note que existem várias configurações de ambiente com suporte a GPU. Para fazer os últimos testes, foi necessário o uso de uma instância paga da plataforma Google Cloud. Essa instância tinha as seguintes configurações:\n",
        "\n",
        "*   Tipo de máquina: n1-standard-8 (8 vCPUs, 30 GB de memória)\n",
        "*   GPUs: 1 x NVIDIA Tesla V100\n",
        "*   Zona: us-west1-b\n",
        "*   Imagem: common-cu101-20191005\n",
        "*   Disco de inicialização: Disco permanente SSD\n",
        "\n",
        "Para conectar o Google Colab a uma instância paga do Google Cloud, siga os passos descritos nesse [vídeo](https://www.youtube.com/watch?v=U5HyNzf_ips).\n",
        "\n",
        "Uma outra forma de tentar lidar com o problema da GPU no Colab é a seguinte:\n",
        "*   Passo 1: Para treinar o modelo de Zaremba et al. [2014], transforme em comentário (coloque # no início de cada linha) o conteúdo da célula que instancia e treina o modelo de Fortunato et al. [2017]. E então execute o notebook.\n",
        "*   Passo 2: Desfaça a ação do passo 1 e se desconecte do atual ambiente de execução (clique no menu 'Runtime' e depois em 'Manage sessions').\n",
        "*   Passo 3: Para treinar o modelo de Fortunato et al. [2017], transforme em comentário (coloque # no início de cada linha) o conteúdo da célula que instancia e treina o modelo de Zaremba et al. [2014]. E então execute o notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqLkUvRJDrM9",
        "colab_type": "text"
      },
      "source": [
        "### **1.3. Bibliotecas e configurações globais**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoOD3C_-WR_H",
        "colab_type": "text"
      },
      "source": [
        "Para executar esse TP, precisamos instalar e importar as seguintes bibliotecas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg8NSUXfUghH",
        "colab_type": "code",
        "outputId": "1363a4b3-67e3-4950-acb4-cb2f74097fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install mxnet-cu101==1.6.0b20190915"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet-cu101==1.6.0b20190915 in ./.local/lib/python3.5/site-packages\r\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in ./.local/lib/python3.5/site-packages (from mxnet-cu101==1.6.0b20190915)\r\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.5/dist-packages (from mxnet-cu101==1.6.0b20190915)\r\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in ./.local/lib/python3.5/site-packages (from mxnet-cu101==1.6.0b20190915)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0b20190915)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0b20190915)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0b20190915)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0b20190915)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xyAhWKG0cs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import re\n",
        "\n",
        "from mxnet import np, npx, nd, context, autograd, gluon, init, util\n",
        "from mxnet.gluon import nn, rnn\n",
        "\n",
        "npx.set_np()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3EQKZXWTsq",
        "colab_type": "text"
      },
      "source": [
        "A classe `Args` declara as configurações globais que nós utilizaremos para implementar e então treinar o modelo de tamanho médio de Zaremba et al. [2014] e o modelo de Fortunato et al. [2017]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cddF7pR31YFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args(object):\n",
        "  # Data settings\n",
        "  archive_file_name = 'ptb.zip'\n",
        "  archive_file_root = './data'\n",
        "  download_root = 'https://github.com/d2l-ai/d2l-en/raw/master/data'\n",
        "  \n",
        "  # Model settings\n",
        "  embedding_size = 650\n",
        "  hidden_size = 650\n",
        "  num_layers = 2\n",
        "  tie_weights = False\n",
        "  save = './model.params'\n",
        "\n",
        "  # Training settings\n",
        "  init_scale = 0.05\n",
        "  num_steps = 35\n",
        "  batch_size = 20\n",
        "  random_shift = False\n",
        "  keep_prob = 0.5\n",
        "  lr_start = 1.0\n",
        "  lr_decay = 0.8\n",
        "  clip_norm = 5\n",
        "  num_epochs = 39\n",
        "  high_lr_epochs = 6\n",
        "\n",
        "  # BBB settings\n",
        "  prior_pi = 0.25\n",
        "  prior_sigma1 = np.exp(-1.0)\n",
        "  prior_sigma2 = np.exp(-7.0)\n",
        "  bbb_on_bias = False\n",
        "\n",
        "  # Inference settings\n",
        "  sample_mode = False\n",
        "  \n",
        "args = Args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5zaBZRG2Scx",
        "colab_type": "text"
      },
      "source": [
        "## **2. Dados de treinamento, validação e teste**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_s6FOv3oXJt",
        "colab_type": "text"
      },
      "source": [
        "Nessa seção do TP, vamos mostrar como obter, pré-processar e carregar em mini-lotes os dados que compõem o *Penn Treebank* (PTB). Esse corpus linquístico é formado por artigos publicados no *Wall Street Journal* (WSJ) e está dividido em subconjuntos de treinamento, validação e teste. A função `read_ptb` é responsável por obter cada um desses subconjuntos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFEAmowj15cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_ptb(subset='train', root=args.archive_file_root):\n",
        "  assert subset in ('train', 'valid', 'test'), \\\n",
        "    \"Invalid subset %s; must be one of ['train', 'valid', 'test']\"%subset\n",
        "\n",
        "  archive_file_path = os.path.join(root, args.archive_file_name)\n",
        "  \n",
        "  if not os.path.isfile(archive_file_path):\n",
        "    \n",
        "    if not os.path.isdir(root):\n",
        "      os.makedirs(root)\n",
        "  \n",
        "    archive_file_url = os.path.join(args.download_root,\n",
        "                                    args.archive_file_name)\n",
        "    urllib.request.urlretrieve(archive_file_url, archive_file_path)\n",
        "    \n",
        "  with zipfile.ZipFile(archive_file_path, mode='r') as f:\n",
        "    data_file_name = 'ptb/ptb.{}.txt'.format(subset)\n",
        "    raw_text = f.read(data_file_name).decode('utf-8').replace('\\n', '<eos>')\n",
        "  \n",
        "  return raw_text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwb_m0f02Kzc",
        "colab_type": "code",
        "outputId": "e47394b4-8585-42b9-a256-651bb83a7cc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_subset, valid_subset, test_subset = [\n",
        "  read_ptb(subset=subset)\n",
        "  for subset in ['train', 'valid', 'test']\n",
        "]\n",
        "\n",
        "'# tokens in train_subset: {}'.format(len(train_subset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# tokens in train_subset: 929589'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwoBBA-YrRMZ",
        "colab_type": "text"
      },
      "source": [
        "Cada subconjunto é, inicialmente, uma sequência de sequências de palavras. Mas a função `read_ptb` o transforma em uma sequência de palavras. Veja, pela saída da célula acima, que o subconjunto de treinamento é representado, dessa forma, por uma sequência de 929.589 palavras.\n",
        "\n",
        "A classe `Vocab`, definida abaixo, representa o conjunto das palavras que compõem um corpus linguístico e fornece duas bijeções. A primeira delas associa, a cada palavra desse conjunto, um número inteiro não-negativo, chamado índice. E a outra é simplesmente a inversa da anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BA3EjBY2OOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "  \n",
        "  def __init__(self, subset):\n",
        "    counter = collections.Counter(subset)\n",
        "    self.token_freqs = sorted(counter.items(), key=lambda x: x[1], \n",
        "                              reverse=True)\n",
        "    self.unk, uniq_tokens = 0, ['<unk>']\n",
        "    uniq_tokens += [token for token, freq in self.token_freqs\n",
        "                    if token not in uniq_tokens]\n",
        "    self.idx_to_token, self.token_to_idx = [], dict()\n",
        "\n",
        "    for token in uniq_tokens:\n",
        "      self.idx_to_token.append(token)\n",
        "      self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "    \n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "\n",
        "    return [self.__getitem__(token) for token in tokens]\n",
        "    \n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "\n",
        "    return [self.idx_to_token[index] for index in indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH3fN1itt0_s",
        "colab_type": "text"
      },
      "source": [
        "Vamos ver quantas palavras existem no vocabulário (ou dicionário) do subconjunto de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2co720Wr29mq",
        "colab_type": "code",
        "outputId": "970b550f-d702-440c-87d3-d9858ddc31ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = Vocab(train_subset)\n",
        "'vocab size: {}'.format(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vocab size: 10000'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-xbI2pLxSWs",
        "colab_type": "text"
      },
      "source": [
        "Agora vamos definir como cada subconjunto do corpus, representado por uma sequência de palavras, deve ser dividido em mini-lotes. Na função abaixo, chamada `bptt_batchify`, o parâmetro `batch_size` indica o número de subsequências da sequência `corpus` em cada mini-lote e o parâmetro `num_steps` assinala a quantidade de palavras por subsequência.\n",
        "\n",
        "Seja $(X, Y)$ um dos mini-lotes retornados por `bptt_batchify`. O valor $X_{ij}$ representa a j-ésima palavra da i-ésima subsequência desse mini-lote. E o valor $Y_{ij}$ indica a palavra subsequente àquela representada por $X_{ij}$ na sequência `corpus`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLXrVA0O3LOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bptt_batchify(corpus, num_steps, batch_size):\n",
        "  num_indices = ((len(corpus) - 1) // batch_size) * batch_size\n",
        "  Xs = np.array(corpus[:num_indices])\n",
        "  Ys = np.array(corpus[1:(num_indices + 1)])\n",
        "  Xs, Ys = Xs.reshape((batch_size, -1)), Ys.reshape((batch_size, -1))\n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "\n",
        "  for i in range(0, num_batches * num_steps, num_steps):\n",
        "    X = Xs[:, i:(i+num_steps)]\n",
        "    Y = Ys[:, i:(i+num_steps)]\n",
        "    # X.shape = Y.shape = (batch_size, num_steps) \n",
        "    yield X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru7CwKja0MXG",
        "colab_type": "text"
      },
      "source": [
        "Verifique, abaixo, que dois mini-lotes adjacentes retornados pela função `bptt_batchify` são também adjacentes na sequência fornecida como entrada. É por isso que o método implementado nessa função é chamado particionamento sequencial (apesar dele não induzir uma partição conforme definição usada em matemática)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P40HPJxj3Sjx",
        "colab_type": "code",
        "outputId": "62ff058f-9218-4bb3-b26f-d915a5b5848a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "my_seq = list(range(30))\n",
        "\n",
        "for X, Y in bptt_batchify(my_seq, num_steps=6, batch_size=2):\n",
        "    print('X:', X, '\\nY:', Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X: [[ 0.  1.  2.  3.  4.  5.]\n",
            " [14. 15. 16. 17. 18. 19.]] \n",
            "Y: [[ 1.  2.  3.  4.  5.  6.]\n",
            " [15. 16. 17. 18. 19. 20.]]\n",
            "X: [[ 6.  7.  8.  9. 10. 11.]\n",
            " [20. 21. 22. 23. 24. 25.]] \n",
            "Y: [[ 7.  8.  9. 10. 11. 12.]\n",
            " [21. 22. 23. 24. 25. 26.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijJ4SHxB6NxG",
        "colab_type": "text"
      },
      "source": [
        "A classe `SeqDataLoader`, definida abaixo, reúne os métodos de pré-processamento e de iteração, via mini-lotes, sobre os dados de um subconjunto do corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSswMnYr3VBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeqDataLoader(object):\n",
        "  \n",
        "  def __init__(self, subset, vocab, num_steps=args.num_steps, \n",
        "               batch_size=args.batch_size, batchify_fn=bptt_batchify,\n",
        "               random_shift=False):\n",
        "    corpus = [vocab[token] for token in subset]\n",
        "    shift = random.randint(0, num_steps) if random_shift else 0\n",
        "    self.corpus = corpus[shift:]\n",
        "    self.vocab = vocab\n",
        "    self.num_steps = num_steps\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = ((len(corpus) - 1) // batch_size) // num_steps\n",
        "    self.get_iter = lambda: batchify_fn(self.corpus, num_steps, batch_size)\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.get_iter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAxcEY3p3jSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = SeqDataLoader(train_subset, vocab, random_shift=args.random_shift)\n",
        "valid_iter = SeqDataLoader(valid_subset, vocab)\n",
        "test_iter = SeqDataLoader(test_subset, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeDSIa66xVj_",
        "colab_type": "text"
      },
      "source": [
        "Pela saída da célula abaixo, podemos ver que cada mini-lote do subconjunto de treinamento contém 20 subsequências de 35 palavras cada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oJY3NRM3meT",
        "colab_type": "code",
        "outputId": "c9fb19ae-2731-4d7c-d533-27544faf3e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for X, Y in train_iter:\n",
        "  print('X.shape:', X.shape, '\\nY.shape:', Y.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (20, 35) \n",
            "Y.shape: (20, 35)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1drO8DKL4Arw",
        "colab_type": "text"
      },
      "source": [
        "## **3. Modelo de Zaremba et al. [2014]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vutakRyiGhde",
        "colab_type": "text"
      },
      "source": [
        "O modelo proposto por Zaremba et al. [2014] consiste em uma camada *embedding* na entrada, um núcleo recorrente de arquitetura LSTM e uma camada densa na saída. Nesse TP, implementaremos apenas a configuração de tamanho médio desse modelo.\n",
        "\n",
        "A camada *embedding* associa, a cada palavra do dicionário de um corpus linguístico, um vetor do espaço coordenado real de dimensão igual a `embedding_size`. Nesse modelo, a camada tem como entrada uma matriz de formato `(num_steps, batch_size)` e retorna um tensor de formato `(num_steps, batch_size, embedding_size)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuKtVi074Mok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = nn.Embedding(input_dim=len(vocab),\n",
        "                       output_dim=args.embedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzrGXWn24-vj",
        "colab_type": "code",
        "outputId": "32692af2-5d56-4f1c-9ace-07261525accd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "encoder.initialize(init.Normal(sigma=args.init_scale), force_reinit=True)\n",
        "encoded = encoder(X.T)\n",
        "'encoded.shape: {}'.format(encoded.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'encoded.shape: (35, 20, 650)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JRz6KyZGjrT",
        "colab_type": "text"
      },
      "source": [
        "O núcleo recorrente desse modelo é formado por `num_layers` camadas LSTM. O espaço-estado de cada uma delas tem dimensão igual a `hidden_size`. Esse núcleo, que modela a dependência sequencial entre as palavras, retorna dois tensores que representam, respectivamente, a saída, de formato `(num_steps, batch_size, hidden_size)`, e o par estado-memória, de formato `(2, num_layers, batch_size, hidden_size)`. Como entrada, ele recebe a saída da camada anterior e o valor inicial do par estado-memória."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRKZlv6O5IcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_layer = rnn.LSTM(hidden_size=args.hidden_size,\n",
        "                      num_layers=args.num_layers,\n",
        "                      dropout=1-args.keep_prob,\n",
        "                      input_size=args.embedding_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_flhGPNV5f3U",
        "colab_type": "code",
        "outputId": "f437c945-e3df-4b16-e67d-fc4d79cbbdab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lstm_layer.initialize(init.Normal(sigma=args.init_scale), force_reinit=True)\n",
        "state = lstm_layer.begin_state(batch_size=args.batch_size)\n",
        "'# states: {}, state[_].shape: {}'.format(len(state), state[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# states: 2, state[_].shape: (2, 20, 650)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu8qAV5v5i2o",
        "colab_type": "code",
        "outputId": "2d22273a-e1c5-495f-84e9-be6b2452ea92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "output, state = lstm_layer(encoded, state)\n",
        "'output.shape: {}'.format(output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'output.shape: (35, 20, 650)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfT2Yn1xGlHc",
        "colab_type": "text"
      },
      "source": [
        "Na saída desse modelo, temos uma camada densa. Ela recebe a saída do núcleo recorrente, após ter o seu formato alterado para `(num_steps * batch_size, hidden_size)`, e então retorna uma matriz de formato `(num_steps * batch_size, vocab_size)`. A aplicação linha por linha da função `softmax` nessa matriz nos dá `num_steps * batch_size` distribuições de probabilidade sobre as palavras do dicionário.\n",
        "\n",
        "A classe `PTBModel` escapsula esse modelo, implementa a sua operação de *forward* e também define a função que retorna o valor inicial do par estado-memória. Como a operação de *backward* é inferida automaticamente pela biblioteca `autograd` do Apache MXNet, nós não precisamos nos preocupar com ela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYj6cyl25yXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PTBModel(nn.HybridBlock):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size=args.embedding_size,\n",
        "               hidden_size=args.hidden_size, num_layers=args.num_layers,\n",
        "               dropout=1-args.keep_prob, tie_weights=args.tie_weights, \n",
        "               **kwargs):\n",
        "    super(PTBModel, self).__init__(**kwargs)\n",
        "\n",
        "    with self.name_scope():\n",
        "      self.drop = nn.Dropout(dropout)\n",
        "      self.encoder = nn.Embedding(input_dim=vocab_size,\n",
        "                                  output_dim=embedding_size)\n",
        "      self.lstm = rnn.LSTM(hidden_size=hidden_size,\n",
        "                           num_layers=num_layers,\n",
        "                           dropout=dropout,\n",
        "                           input_size=embedding_size)\n",
        "      \n",
        "      if tie_weights:\n",
        "        self.decoder = nn.Dense(vocab_size, in_units=hidden_size,\n",
        "                                params=self.encoder.params)\n",
        "      else:\n",
        "        self.decoder = nn.Dense(vocab_size, in_units=hidden_size)\n",
        "            \n",
        "      self.hidden_size = hidden_size\n",
        "\n",
        "  def hybrid_forward(self, F, inputs, state):\n",
        "    # inputs.shape = (batch_size, num_steps)\n",
        "    # encoded.shape = (num_steps, batch_size, embedding_size)\n",
        "    encoded = self.drop(self.encoder(inputs.T))\n",
        "    # output.shape = (num_steps, batch_size, hidden_size)\n",
        "    # state[_].shape = (num_layers, batch_size, hidden_size)\n",
        "    output, state = self.lstm(encoded, state)\n",
        "    output = self.drop(output)\n",
        "    # decoded.shape = (num_steps * batch_size, vocab_size)\n",
        "    decoded = self.decoder(output.reshape((-1, self.hidden_size)))\n",
        "    return decoded, state\n",
        "    \n",
        "  def begin_state(self, *args, **kwargs):\n",
        "    return self.lstm.begin_state(*args, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT4AzfESGl84",
        "colab_type": "text"
      },
      "source": [
        "RNNs são propensas a problemas de extinção e explosão de gradientes. O primeiro deles pode ser remediado com o uso da arquitetura LSTM. E o segundo pode ser controlado com a utilização da função `grad_clipping`, definida abaixo, e do método BPTT truncado, que está implícito na forma como nós dividimos os subconjuntos do corpus em mini-lotes e no treinamento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNkUOwxQ6o-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grad_clipping(model, clip_norm):\n",
        "  params = [p.data() for p in model.collect_params().values()]\n",
        "  norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))\n",
        "\n",
        "  if norm > clip_norm:\n",
        "    for param in params:\n",
        "      param.grad[:] *= clip_norm / norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CprIOOTqGmnM",
        "colab_type": "text"
      },
      "source": [
        "O treinamento desse modelo está implementado nas funções `train`, `train_epoch` e `eval`, definidas nas três células a seguir. A principal estatística que elas retornam ou imprimem é a [perplexidade por palavra](https://en.wikipedia.org/wiki/Perplexity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjtSD_226ujU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, data_iter, loss, ctx):\n",
        "  num_steps = data_iter.num_steps\n",
        "  batch_size = data_iter.batch_size\n",
        "  state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
        "  loss_sum = 0\n",
        "  steps_sum = 0\n",
        "\n",
        "  for X, Y in data_iter:\n",
        "    X = X.as_in_context(ctx)\n",
        "    # Y.shape = (batch_size, num_steps)\n",
        "    y = Y.T.reshape((-1,))\n",
        "    # y.shape = (num_steps * batch_size)\n",
        "    y = y.as_in_context(ctx)\n",
        "    yhat, state = model(X, state)\n",
        "    L = loss(yhat, y)\n",
        "    # Sum over the sequence\n",
        "    sequence_neg_log_prob = L.reshape((batch_size, num_steps)).sum(axis=1)\n",
        "    # Average over the batch\n",
        "    data_loss = sequence_neg_log_prob.mean()\n",
        "    loss_sum += data_loss\n",
        "    steps_sum += num_steps\n",
        "\n",
        "  return loss_sum / steps_sum"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMH9QZdK611J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(model, train_iter, loss, clip_norm, trainer, ctx):\n",
        "  start_time = time.time()\n",
        "  num_steps = train_iter.num_steps\n",
        "  batch_size = train_iter.batch_size\n",
        "  state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
        "  loss_sum = 0\n",
        "  steps_sum = 0\n",
        "  \n",
        "  for X, Y in train_iter:\n",
        "\n",
        "    for s in state: s.detach()\n",
        "    \n",
        "    X = X.as_in_context(ctx)\n",
        "    # Y.shape = (batch_size, num_steps)\n",
        "    y = Y.T.reshape((-1,))\n",
        "    # y.shape = (num_steps * batch_size)\n",
        "    y = y.as_in_context(ctx)\n",
        "\n",
        "    with autograd.record():\n",
        "      yhat, state = model(X, state)\n",
        "      L = loss(yhat, y)\n",
        "      # Sum over the sequence\n",
        "      sequence_neg_log_prob = L.reshape((batch_size, num_steps)).sum(axis=1)\n",
        "      # Average over the batch\n",
        "      data_loss = sequence_neg_log_prob.mean()\n",
        "    \n",
        "    data_loss.backward()\n",
        "    grad_clipping(model, clip_norm)\n",
        "    trainer.step(batch_size=1)\n",
        "    loss_sum += data_loss\n",
        "    steps_sum += num_steps\n",
        "        \n",
        "  return loss_sum / steps_sum, time.time() - start_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPqNBeI7nAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_iter, valid_iter, test_iter, init_scale=args.init_scale, \n",
        "          lr=args.lr_start, lr_decay=args.lr_decay, num_epochs=args.num_epochs, \n",
        "          high_lr_epochs=args.high_lr_epochs, clip_norm=args.clip_norm, \n",
        "          ctx=context.cpu()):\n",
        "  loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "  model.initialize(ctx=ctx, force_reinit=True, \n",
        "                   init=init.Normal(sigma=init_scale))\n",
        "  trainer = gluon.Trainer(model.collect_params(), 'sgd', \n",
        "                          {'learning_rate': lr})\n",
        "  model.hybridize()\n",
        "    \n",
        "  # Train and check the progress\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    if epoch >= high_lr_epochs:\n",
        "      lr = lr * lr_decay\n",
        "      trainer._init_optimizer('sgd', {'learning_rate': lr})\n",
        "\n",
        "    train_loss, speed = train_epoch(model, train_iter, loss, clip_norm, \n",
        "                                    trainer, ctx)\n",
        "    print('[Epoch %d] time cost %.2fs, train loss %.2f, train ppl %.2f'%(\n",
        "          epoch, speed, train_loss, math.exp(train_loss)))\n",
        "    valid_loss = eval(model, valid_iter, loss, ctx)\n",
        "    print('valid loss %.2f, valid ppl %.2f'%(valid_loss, math.exp(valid_loss)))\n",
        "    test_loss = eval(model, test_iter, loss, ctx)\n",
        "    print('test loss %.2f, test ppl %.2f'%(test_loss, math.exp(test_loss)))\n",
        "  \n",
        "  model.hybridize(active=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7iDDi3G8O1A",
        "colab_type": "code",
        "outputId": "1b531a90-2a90-42ab-b844-60e3e9220c66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def try_gpu(device_id=0):\n",
        "  if context.num_gpus() >= device_id + 1:\n",
        "    return context.gpu(device_id)\n",
        "  else:\n",
        "    return context.cpu()\n",
        "\n",
        "ctx = try_gpu()\n",
        "print(ctx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpu(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKtSmPx8Gv0V",
        "colab_type": "text"
      },
      "source": [
        "A seguir, vamos instanciar o modelo e executar o seu treinamento. No artigo de Zaremba et al. [2014], a configuração de tamanho médio desse modelo obteve perplexidades por palavra de 86.2 e 82.7 nos subconjuntos de validação e teste, respectivamente. \n",
        "\n",
        "Uma vez que os pesos do modelo são inicializados de forma aleatória, os resultados podem variar cada vez que executamos o treinamento. O melhor resultado que obtivemos nesse notebook foi de 86.04 e 81.87 nos mesmos subconjuntos, nessa ordem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHfMBNWW8YXT",
        "colab_type": "code",
        "outputId": "c37e88c4-e2b7-4013-c1ec-3eec6a01fec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = PTBModel(len(vocab))\n",
        "train(model, train_iter, valid_iter, test_iter, ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0] time cost 22.38s, train loss 5.91, train ppl 367.70\n",
            "valid loss 5.36, valid ppl 211.76\n",
            "test loss 5.33, test ppl 207.05\n",
            "[Epoch 1] time cost 22.08s, train loss 5.19, train ppl 179.45\n",
            "valid loss 5.05, valid ppl 155.49\n",
            "test loss 5.03, test ppl 152.29\n",
            "[Epoch 2] time cost 21.87s, train loss 4.94, train ppl 140.39\n",
            "valid loss 4.88, valid ppl 131.68\n",
            "test loss 4.86, test ppl 128.88\n",
            "[Epoch 3] time cost 21.99s, train loss 4.78, train ppl 119.35\n",
            "valid loss 4.78, valid ppl 119.08\n",
            "test loss 4.76, test ppl 117.02\n",
            "[Epoch 4] time cost 21.86s, train loss 4.66, train ppl 105.65\n",
            "valid loss 4.70, valid ppl 109.80\n",
            "test loss 4.68, test ppl 107.53\n",
            "[Epoch 5] time cost 21.97s, train loss 4.57, train ppl 96.10\n",
            "valid loss 4.65, valid ppl 104.07\n",
            "test loss 4.62, test ppl 101.82\n",
            "[Epoch 6] time cost 21.95s, train loss 4.45, train ppl 85.39\n",
            "valid loss 4.59, valid ppl 98.32\n",
            "test loss 4.56, test ppl 95.85\n",
            "[Epoch 7] time cost 21.88s, train loss 4.35, train ppl 77.31\n",
            "valid loss 4.55, valid ppl 94.68\n",
            "test loss 4.52, test ppl 92.13\n",
            "[Epoch 8] time cost 21.95s, train loss 4.26, train ppl 70.96\n",
            "valid loss 4.52, valid ppl 92.06\n",
            "test loss 4.49, test ppl 89.31\n",
            "[Epoch 9] time cost 21.95s, train loss 4.20, train ppl 66.38\n",
            "valid loss 4.51, valid ppl 90.67\n",
            "test loss 4.48, test ppl 87.88\n",
            "[Epoch 10] time cost 21.86s, train loss 4.14, train ppl 62.52\n",
            "valid loss 4.49, valid ppl 89.54\n",
            "test loss 4.46, test ppl 86.66\n",
            "[Epoch 11] time cost 22.02s, train loss 4.09, train ppl 59.63\n",
            "valid loss 4.49, valid ppl 88.73\n",
            "test loss 4.45, test ppl 85.68\n",
            "[Epoch 12] time cost 21.97s, train loss 4.05, train ppl 57.41\n",
            "valid loss 4.48, valid ppl 88.24\n",
            "test loss 4.44, test ppl 84.99\n",
            "[Epoch 13] time cost 21.89s, train loss 4.02, train ppl 55.49\n",
            "valid loss 4.48, valid ppl 87.88\n",
            "test loss 4.44, test ppl 84.57\n",
            "[Epoch 14] time cost 21.89s, train loss 3.99, train ppl 54.06\n",
            "valid loss 4.47, valid ppl 87.65\n",
            "test loss 4.43, test ppl 84.11\n",
            "[Epoch 15] time cost 22.10s, train loss 3.97, train ppl 53.00\n",
            "valid loss 4.47, valid ppl 87.38\n",
            "test loss 4.43, test ppl 83.72\n",
            "[Epoch 16] time cost 22.03s, train loss 3.95, train ppl 52.07\n",
            "valid loss 4.47, valid ppl 87.24\n",
            "test loss 4.43, test ppl 83.54\n",
            "[Epoch 17] time cost 21.93s, train loss 3.94, train ppl 51.35\n",
            "valid loss 4.47, valid ppl 86.94\n",
            "test loss 4.42, test ppl 83.18\n",
            "[Epoch 18] time cost 21.95s, train loss 3.93, train ppl 50.71\n",
            "valid loss 4.47, valid ppl 86.93\n",
            "test loss 4.42, test ppl 83.14\n",
            "[Epoch 19] time cost 22.17s, train loss 3.92, train ppl 50.31\n",
            "valid loss 4.46, valid ppl 86.83\n",
            "test loss 4.42, test ppl 82.93\n",
            "[Epoch 20] time cost 21.90s, train loss 3.91, train ppl 49.92\n",
            "valid loss 4.46, valid ppl 86.67\n",
            "test loss 4.42, test ppl 82.75\n",
            "[Epoch 21] time cost 21.92s, train loss 3.90, train ppl 49.54\n",
            "valid loss 4.46, valid ppl 86.70\n",
            "test loss 4.42, test ppl 82.74\n",
            "[Epoch 22] time cost 22.00s, train loss 3.90, train ppl 49.48\n",
            "valid loss 4.46, valid ppl 86.53\n",
            "test loss 4.41, test ppl 82.54\n",
            "[Epoch 23] time cost 21.94s, train loss 3.90, train ppl 49.18\n",
            "valid loss 4.46, valid ppl 86.51\n",
            "test loss 4.41, test ppl 82.51\n",
            "[Epoch 24] time cost 21.94s, train loss 3.89, train ppl 49.03\n",
            "valid loss 4.46, valid ppl 86.47\n",
            "test loss 4.41, test ppl 82.45\n",
            "[Epoch 25] time cost 21.94s, train loss 3.89, train ppl 48.88\n",
            "valid loss 4.46, valid ppl 86.35\n",
            "test loss 4.41, test ppl 82.32\n",
            "[Epoch 26] time cost 21.99s, train loss 3.89, train ppl 48.73\n",
            "valid loss 4.46, valid ppl 86.39\n",
            "test loss 4.41, test ppl 82.34\n",
            "[Epoch 27] time cost 21.91s, train loss 3.89, train ppl 48.75\n",
            "valid loss 4.46, valid ppl 86.30\n",
            "test loss 4.41, test ppl 82.25\n",
            "[Epoch 28] time cost 21.94s, train loss 3.88, train ppl 48.62\n",
            "valid loss 4.46, valid ppl 86.29\n",
            "test loss 4.41, test ppl 82.23\n",
            "[Epoch 29] time cost 22.11s, train loss 3.88, train ppl 48.59\n",
            "valid loss 4.46, valid ppl 86.25\n",
            "test loss 4.41, test ppl 82.18\n",
            "[Epoch 30] time cost 22.05s, train loss 3.88, train ppl 48.57\n",
            "valid loss 4.46, valid ppl 86.25\n",
            "test loss 4.41, test ppl 82.17\n",
            "[Epoch 31] time cost 21.99s, train loss 3.88, train ppl 48.45\n",
            "valid loss 4.46, valid ppl 86.25\n",
            "test loss 4.41, test ppl 82.15\n",
            "[Epoch 32] time cost 22.05s, train loss 3.88, train ppl 48.45\n",
            "valid loss 4.46, valid ppl 86.25\n",
            "test loss 4.41, test ppl 82.16\n",
            "[Epoch 33] time cost 21.97s, train loss 3.88, train ppl 48.43\n",
            "valid loss 4.46, valid ppl 86.23\n",
            "test loss 4.41, test ppl 82.14\n",
            "[Epoch 34] time cost 21.93s, train loss 3.88, train ppl 48.47\n",
            "valid loss 4.46, valid ppl 86.21\n",
            "test loss 4.41, test ppl 82.12\n",
            "[Epoch 35] time cost 21.98s, train loss 3.88, train ppl 48.43\n",
            "valid loss 4.46, valid ppl 86.20\n",
            "test loss 4.41, test ppl 82.11\n",
            "[Epoch 36] time cost 21.91s, train loss 3.88, train ppl 48.41\n",
            "valid loss 4.46, valid ppl 86.19\n",
            "test loss 4.41, test ppl 82.10\n",
            "[Epoch 37] time cost 21.90s, train loss 3.88, train ppl 48.41\n",
            "valid loss 4.46, valid ppl 86.19\n",
            "test loss 4.41, test ppl 82.10\n",
            "[Epoch 38] time cost 21.90s, train loss 3.88, train ppl 48.46\n",
            "valid loss 4.46, valid ppl 86.18\n",
            "test loss 4.41, test ppl 82.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9H-4XfWskZ9",
        "colab_type": "text"
      },
      "source": [
        "Um fato curioso é que o treinamento desse modelo pelos seus autores (que na época trabalhavam no Google Brain), no ano de 2014, consumiu meio dia de processamento, mesmo usando uma GPU. Hoje, utilizando um ambiente gratuito, executamos o mesmo treinamento em cerca de 23 minutos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCfbN_DhDIUA",
        "colab_type": "text"
      },
      "source": [
        "## **4. Abordagem bayesiana para redes neurais**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REXGXYD3h3Ce",
        "colab_type": "text"
      },
      "source": [
        "A abordagem bayesiana para redes neurais tem por finalidade representar a incerteza sistemática nessa classe de modelos. Ela consiste em atribuir ao vetor de pesos $\\mathbf{w}$ da rede, que é uma constante numérica desconhecida, uma distribuição de probabilidade para expressar nossa expectativa, com base na informação disponível, de que dado valor para $\\mathbf{w}$ é o verdadeiro. E ela nos ajuda a responder às seguintes perguntas: tendo em vista o tamanho e a diversidade do conjunto de treinamento, qual é a incerteza relacionada à estrutura e aos pesos de uma rede neural treinada nesse conjunto e qual é a confiança dessa rede ao fazer cada previsão?\n",
        "\n",
        "Nessa abordagem, o treinamento de uma rede neural passa a ter como objetivo calcular a distribuição a posteriori de $\\mathbf{w}$ dado o conjunto de treinamento $\\mathcal{D}$, isto é, a distribuição $p(\\mathbf{w} \\mid \\mathcal{D})$. E a distribuição preditiva de $\\mathbf{y} \\mid \\mathbf{x}$ fica então definida por\n",
        "\n",
        "$$\\mathbb{E}_{p(\\mathbf{w} \\mid \\mathcal{D})}[p(\\mathbf{y} \\mid \\mathbf{x}, \\mathbf{w})].$$\n",
        "\n",
        "Como é inviável obter a posteriori $p(\\mathbf{w} \\mid \\mathcal{D})$ em redes neurais de qualquer tamanho prático, Blundell et al. [2015] sugerem considerar uma família $\\mathcal{Q} = {\\{q(\\mathbf{w} \\mid \\theta) : \\theta \\in \\Theta\\}}$ de distribuição paramétrica e então encontrar os parâmetros $\\theta$ da distribuição $q(\\mathbf{w} \\mid \\theta)$ que minimizam a sua divergência de Kullback-Leibler ($\\text{KL}$) com a posteriori $p(\\mathbf{w} \\mid \\mathcal{D})$:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\theta^{*}\n",
        "  & = \\underset{\\theta}{\\operatorname{arg min}} \\text{KL}(q(\\mathbf{w} \\mid \\theta) \\Vert p(\\mathbf{w} \\mid \\mathcal{D})) \\\\\n",
        "  & = \\underset{\\theta}{\\operatorname{arg min}} \\int q(\\mathbf{w} \\mid \\theta) \\log \\frac{q(\\mathbf{w} \\mid \\theta)} {p(\\mathbf{w} \\mid \\mathcal{D})}d\\mathbf{w} \\\\\n",
        "  & = \\underset{\\theta}{\\operatorname{arg min}} \\int q(\\mathbf{w} \\mid \\theta) \\left[\\log q(\\mathbf{w} \\mid \\theta) - \\frac{\\log p(\\mathcal{D} \\mid \\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})}\\right]d\\mathbf{w} \\\\\n",
        "  & = \\underset{\\theta}{\\operatorname{arg min}} \\int q(\\mathbf{w} \\mid \\theta) \\left[\\log \\frac{q(\\mathbf{w} \\mid \\theta)}{p(\\mathbf{w})} - \\log p(\\mathcal{D} \\mid \\mathbf{w}) + \\log p(\\mathcal{D})\\right]d\\mathbf{w} \\\\\n",
        "  & = \\underset{\\theta}{\\operatorname{arg min}} \\text{KL}(q(\\mathbf{w} \\mid \\theta) \\Vert p(\\mathbf{w})) - \\mathbb{E}_{q(\\mathbf{w} \\mid \\theta)}[\\log p(\\mathcal{D} \\mid \\mathbf{w})] + \\log p(\\mathcal{D}) \\\\\n",
        "  & = \\underset{\\theta}{\\operatorname{arg min}}\\text{KL}(q(\\mathbf{w} \\mid \\theta) \\Vert p(\\mathbf{w})) - \\mathbb{E}_{q(\\mathbf{w} \\mid \\theta)}[\\log p(\\mathcal{D} \\mid \\mathbf{w})], \\\\\n",
        "\\end{aligned}$$\n",
        "\n",
        "onde o termo $\\text{KL}(q(\\mathbf{w} \\mid \\theta) \\Vert p(\\mathbf{w}))$ é chamado custo da complexidade e atua como um regularizador, e o termo $- \\mathbb{E}_{q(\\mathbf{w} \\mid \\theta)}[\\log p(\\mathcal{D} \\mid \\mathbf{w})]$ recebe o nome de custo da log-verossimilhança. Note que o primeiro termo não depende da saída da rede neural. Se assumirmos que os valores de $\\mathbf{w}$ são coletivamente independentes, como é o caso, então podemos calcular o custo da complexidade camada por camada.\n",
        "\n",
        "O modelo de Fortunato et al. [2017] fixa que a aproximação variacional $q(\\mathbf{w} \\mid \\theta)$ é uma distribuição normal multivariada diagonal com parâmetros $\\theta$ e que a distribuição a priori $p(\\mathbf{w})$ é uma mistura de normais multivariadas diagonais, centradas na origem, com parâmetros `priori_pi`, `priori_sigma1` e `priori_sigma2`. As classes `CustomNormal` e `CustomScaleMixture` definem essas duas distribuições. \n",
        "\n",
        "Note que usamos `rho` ao invés de `sigma`, respectivamente $\\rho$ e $\\sigma$, para parametrizar uma distribuição normal multivariada. Nesse caso, definimos $\\sigma = \\log(1 + \\exp(\\rho))$, donde sempre vale $\\sigma > 0$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RRjSp16cIrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomNormal(object):\n",
        "  \n",
        "  def __init__(self, F, mu, rho, shape=None):\n",
        "    super(CustomNormal).__init__()\n",
        "    self.mu = mu\n",
        "    self.rho = rho\n",
        "    \n",
        "    if F is nd:\n",
        "      self.normal = lambda : F.np.random.normal(size=rho.shape, ctx=rho.ctx)\n",
        "    else:\n",
        "      self.normal = lambda : F.np.random.normal(size=shape)\n",
        "    \n",
        "    self.log1p = F.np.log1p\n",
        "    self.exp = F.np.exp\n",
        "    self.log = F.np.log\n",
        "  \n",
        "  @property\n",
        "  def sigma(self):\n",
        "    return self.log1p(self.exp(self.rho))\n",
        "  \n",
        "  def sample(self):\n",
        "    epsilon = self.normal()\n",
        "    return self.mu + self.sigma * epsilon\n",
        "\n",
        "  def _squared_difference(self, x, y):\n",
        "    return (x - y) ** 2\n",
        "  \n",
        "  def log_prob(self, x):\n",
        "    return (-0.5 * self.log(2. * math.pi) - self.log(self.sigma)\n",
        "            -0.5 * self._squared_difference(x / self.sigma, \n",
        "                                            self.mu / self.sigma))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ls6X-BNRJaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomScaleMixture(object):\n",
        "\n",
        "  def __init__(self, F, pi, sigma1, sigma2, ctx=None, dtype=None):\n",
        "    super(CustomScaleMixture).__init__()\n",
        "\n",
        "    if F is nd:\n",
        "      to_array = lambda v : F.array(v, ctx=ctx, dtype=dtype).as_np_ndarray()\n",
        "    else:\n",
        "      to_array = lambda v : v\n",
        "    \n",
        "    self.log = F.np.log\n",
        "    self.exp = F.np.exp\n",
        "    self.max = F.np.max\n",
        "    self.sum = F.np.sum\n",
        "    self.squeeze = F.np.squeeze\n",
        "    self.stack = F.np.stack\n",
        "    \n",
        "    self.mu, self.pi, self.sigma1, self.sigma2 = (\n",
        "        to_array(v) for v in (0.0, pi, sigma1, sigma2))\n",
        "    \n",
        "    rho1 = self.log(self.exp(self.sigma1) - 1.0)\n",
        "    rho2 = self.log(self.exp(self.sigma2) - 1.0)\n",
        "    self.n1 = CustomNormal(F, self.mu, rho1)\n",
        "    self.n2 = CustomNormal(F, self.mu, rho2)\n",
        "\n",
        "  # This function is more numerically stable than log(sum(exp(x))).\n",
        "  def _log_sum_exp(self, x, axis, keepdims=False):\n",
        "    max_x = self.max(x, axis=axis, keepdims=True)\n",
        "    x = self.log(self.sum(self.exp(x - max_x), axis=axis, keepdims=True))\n",
        "    x = x + max_x\n",
        "\n",
        "    if not keepdims:\n",
        "      x = self.squeeze(x, axis=axis)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  def log_prob(self, x):\n",
        "    mix1 = self.sum(self.n1.log_prob(x), -1) + self.log(self.pi)\n",
        "    mix2 = self.sum(self.n2.log_prob(x), -1) + self.log(1.0 - self.pi)\n",
        "    prior_mix = self.stack([mix1, mix2])\n",
        "    lse_mix = self._log_sum_exp(prior_mix, [0])\n",
        "    return self.sum(lse_mix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNG-x9aYRYxG",
        "colab_type": "text"
      },
      "source": [
        "Para inicializar os parâmetros relacionados a $\\rho$ nas camadas bayesianas, precisamos definir a classe `Uniform` e as funções `non_lstm_rho_initializer` e `lstm_rho_initializer` conforme especificado por Fortunato et al. [2017]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsjKKw16EtZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Uniform(init.Initializer):\n",
        "  \n",
        "  def __init__(self, low=-0.07, high=0.07):\n",
        "    super(Uniform, self).__init__(low=low, high=high)\n",
        "    self.low = low\n",
        "    self.high = high\n",
        "\n",
        "  def _init_weight(self, _, arr):\n",
        "    np.random.uniform(self.low, self.high, arr.shape, out=arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y1loOPo2VFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def non_lstm_rho_initializer(prior_pi, prior_sigma1, prior_sigma2):\n",
        "  prior_sigma = np.sqrt(prior_pi * (prior_sigma1 ** 2) + \n",
        "                        (1 - prior_pi) * (prior_sigma2 ** 2))\n",
        "  minval = np.log(np.exp(prior_sigma / 2.0) - 1.0)\n",
        "  maxval = np.log(np.exp(prior_sigma / 1.0) - 1.0)\n",
        "  return Uniform(minval, maxval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-H-1zQz2U3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_rho_initializer(prior_pi, prior_sigma1, prior_sigma2):\n",
        "  prior_sigma = np.sqrt(prior_pi * (prior_sigma1 ** 2) + \n",
        "                        (1 - prior_pi) * (prior_sigma2 ** 2))\n",
        "  minval = np.log(np.exp(prior_sigma / 4.0) - 1.0)\n",
        "  maxval = np.log(np.exp(prior_sigma / 2.0) - 1.0)\n",
        "  return Uniform(minval, maxval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKViEXoIo3lB",
        "colab_type": "text"
      },
      "source": [
        "### **4.1 Camadas bayesianas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OBnIQaOmxGh",
        "colab_type": "text"
      },
      "source": [
        "Para transformar o modelo de Zaremba et al. [2014] no modelo de Fortunato et al. [2017], precisamos definir uma versão bayesiana das camadas *embedding*, LSTM e densa. Para tanto, seguiremos os passos descritos nesse [artigo](https://mxnet.incubator.apache.org/api/python/docs/tutorials/extend/custom_layer.html) e utilizaremos, como ponto de partida, o código-fonte da versão original dessas camadas.\n",
        "\n",
        "Nessa nova versão, os parâmetros de cada camada não são mais os seus respectivos pesos, mas sim os parâmetros da aproximação variacional da distribuição a posteriori desses pesos. Veja isso no construtor `__init__` de cada uma das três classes definidas a seguir. Porém, a principal diferença dessa versão em relação à original diz respeito à operação de *forward*. O novo procedimento pode ser resumido da seguinte forma:\n",
        "\n",
        "*   **Passo 1**: Obtenha os pesos da camada. No modo treino ou no modo simulação, amostre os pesos da aproximação variacional. Caso contrário, tome o vetor de médias dessa distribuição.\n",
        "*   **Passo 2**: Execute a operação de *forward* tal como na versão original da camada, usando os pesos obtidos no passo 1.\n",
        "*   **Passo 3**: Calcule o custo da complexidade da camada, isto é, uma estimativa da divergência $\\text{KL}$ da distribuição a priori dos pesos em relação à aproximação variacional, usando os pesos obtidos no passo 1.\n",
        "*   **Passo 4**: Salve o custo da complexidade calculado no passo 3.\n",
        "*   **Passo 5**: Retorne o resultado da operação executada no passo 2.\n",
        "\n",
        "Essa operação de *forward* está implementada nos métodos `forward`, `hybrid_forward`[, `_forward_kernel`] e `_get_total_kl_cost` das classes definidas nessa seção.\n",
        "\n",
        "Segue, na célula abaixo, a definição da classe `BayesianEmbedding`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIJZJehHl465",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianEmbedding(nn.HybridBlock):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim, prior_pi, prior_sigma1, \n",
        "               prior_sigma2, dtype='float32', weight_mu_initializer=None, \n",
        "               weight_rho_initializer=None, sample_mode=False, **kwargs):\n",
        "    super(BayesianEmbedding, self).__init__(**kwargs)\n",
        "    self._input_dim = input_dim\n",
        "    self._output_dim = output_dim\n",
        "    self._prior_pi = prior_pi\n",
        "    self._prior_sigma1 = prior_sigma1\n",
        "    self._prior_sigma2 = prior_sigma2\n",
        "    self._sample_mode = sample_mode\n",
        "    self._total_kl_cost = None\n",
        "    self._kwargs = {'input_dim': input_dim, 'output_dim': output_dim,\n",
        "                    'dtype': dtype, 'sparse_grad': False}\n",
        "    \n",
        "    with self.name_scope():\n",
        "      self.weight_mu = self.params.get('weight_mu',\n",
        "                                       shape=(input_dim, output_dim),\n",
        "                                       init=weight_mu_initializer,\n",
        "                                       dtype=dtype, allow_deferred_init=True)\n",
        "      self.weight_rho = self.params.get('weight_rho',\n",
        "                                        shape=(input_dim, output_dim),\n",
        "                                        init=weight_rho_initializer,\n",
        "                                        dtype=dtype, allow_deferred_init=True)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    s = '{name}({input_dim} -> {output_dim}, {dtype})'\n",
        "    return s.format(name=self.__class__.__name__, **self._kwargs)\n",
        "\n",
        "  def forward(self, x, *args):\n",
        "    emb, total_kl_cost = super(BayesianEmbedding, self).forward(x, *args)\n",
        "    self._total_kl_cost = total_kl_cost\n",
        "    return emb\n",
        "  \n",
        "  def hybrid_forward(self, F, x, weight_mu, weight_rho):\n",
        "    weight_dist = CustomNormal(F, weight_mu, weight_rho, self.weight_rho.shape)\n",
        "\n",
        "    if autograd.is_training() or self._sample_mode:\n",
        "      weight = weight_dist.sample()\n",
        "    else:\n",
        "      weight = weight_dist.mu\n",
        "    \n",
        "    emb = F.npx.embedding(x, weight, name='fwd', **self._kwargs)\n",
        "    # We could save computation here.\n",
        "    total_kl_cost = self._get_total_kl_cost(F, weight_dist, weight)\n",
        "\n",
        "    return emb, total_kl_cost\n",
        "  \n",
        "  def kl_cost(self, scale=1.0):\n",
        "    assert self._total_kl_cost is not None, \\\n",
        "      'You must execute a forward operation before getting the KL cost'\n",
        "    \n",
        "    return self._total_kl_cost * scale\n",
        "    \n",
        "  def _get_total_kl_cost(self, F, weight_dist, weight):\n",
        "    if F is nd:\n",
        "      ctx = self.weight_mu.list_ctx()[0]\n",
        "      dtype = self.weight_mu.dtype\n",
        "      prior = CustomScaleMixture(F, self._prior_pi, self._prior_sigma1, \n",
        "                                self._prior_sigma2, ctx, dtype)\n",
        "    else:\n",
        "      prior = CustomScaleMixture(F, self._prior_pi, self._prior_sigma1, \n",
        "                                self._prior_sigma2)\n",
        "    \n",
        "    log_prior = prior.log_prob(weight)\n",
        "    log_variational_posterior = weight_dist.log_prob(weight).sum()\n",
        "    return log_variational_posterior - log_prior\n",
        "\n",
        "  @property\n",
        "  def sample_mode(self):\n",
        "    return self._sample_mode\n",
        "  \n",
        "  @sample_mode.setter\n",
        "  def sample_mode(self, value):\n",
        "    self._sample_mode = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlXvlbRs0FR8",
        "colab_type": "text"
      },
      "source": [
        "Passemos, agora, para a definição da classe `BayesianLSTM`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2bsYDIEAd6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianLSTM(nn.HybridBlock):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, prior_pi, prior_sigma1,\n",
        "               prior_sigma2, num_layers=1, bidirectional=False, dtype='float32',\n",
        "               i2h_weight_mu_initializer=None, i2h_weight_rho_initializer=None,\n",
        "               h2h_weight_mu_initializer=None, h2h_weight_rho_initializer=None,\n",
        "               i2h_bias_mu_initializer='zeros', i2h_bias_rho_initializer=None,\n",
        "               h2h_bias_mu_initializer='zeros', h2h_bias_rho_initializer=None,\n",
        "               sample_mode=False, **kwargs):\n",
        "    super(BayesianLSTM, self).__init__(**kwargs)\n",
        "    self._input_size = input_size\n",
        "    self._hidden_size = hidden_size\n",
        "    self._prior_pi = prior_pi\n",
        "    self._prior_sigma1 = prior_sigma1\n",
        "    self._prior_sigma2 = prior_sigma2\n",
        "    self._num_layers = num_layers\n",
        "    self._dir = 2 if bidirectional else 1\n",
        "    self._dtype = dtype\n",
        "    self._i2h_weight_mu_initializer = i2h_weight_mu_initializer\n",
        "    self._i2h_weight_rho_initializer = i2h_weight_rho_initializer\n",
        "    self._h2h_weight_mu_initializer = h2h_weight_mu_initializer\n",
        "    self._h2h_weight_rho_initializer = h2h_weight_rho_initializer\n",
        "    self._i2h_bias_mu_initializer = i2h_bias_mu_initializer\n",
        "    self._i2h_bias_rho_initializer = i2h_bias_rho_initializer\n",
        "    self._h2h_bias_mu_initializer = h2h_bias_mu_initializer\n",
        "    self._h2h_bias_rho_initializer = h2h_bias_rho_initializer\n",
        "    self._sample_mode = sample_mode\n",
        "    self._params_shape = None\n",
        "    self._total_kl_cost = None\n",
        "    self._gates = 4 # number of gates in a LSTM layer\n",
        "    self._layout = 'TNC' # T, N and C stand for sequence length, batch size, ... \n",
        "                         # and feature dimensions respectively.\n",
        "\n",
        "    ng, ni, nh = self._gates, input_size, hidden_size\n",
        "    for i in range(num_layers):\n",
        "      for j in ['l', 'r'][:self._dir]:\n",
        "        self._register_param('{}{}_i2h_weight_mu'.format(j, i),\n",
        "                             shape=(ng*nh, ni),\n",
        "                             init=i2h_weight_mu_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_i2h_weight_rho'.format(j, i),\n",
        "                             shape=(ng*nh, ni),\n",
        "                             init=i2h_weight_rho_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_h2h_weight_mu'.format(j, i),\n",
        "                             shape=(ng*nh, nh),\n",
        "                             init=h2h_weight_mu_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_h2h_weight_rho'.format(j, i),\n",
        "                             shape=(ng*nh, nh),\n",
        "                             init=h2h_weight_rho_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_i2h_bias_mu'.format(j, i),\n",
        "                             shape=(ng*nh,),\n",
        "                             init=i2h_bias_mu_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_i2h_bias_rho'.format(j, i),\n",
        "                             shape=(ng*nh,),\n",
        "                             init=i2h_bias_rho_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_h2h_bias_mu'.format(j, i),\n",
        "                             shape=(ng*nh,),\n",
        "                             init=h2h_bias_mu_initializer, dtype=dtype)\n",
        "        self._register_param('{}{}_h2h_bias_rho'.format(j, i),\n",
        "                             shape=(ng*nh,),\n",
        "                             init=h2h_bias_rho_initializer, dtype=dtype)\n",
        "      \n",
        "      ni = nh * self._dir\n",
        "\n",
        "  def _register_param(self, name, shape, init, dtype):\n",
        "    p = self.params.get(name, shape=shape, init=init,\n",
        "                        allow_deferred_init=True, dtype=dtype)\n",
        "    setattr(self, name, p)\n",
        "    return p\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = '{name}({mapping}, {_layout}'\n",
        "\n",
        "    if self._num_layers != 1:\n",
        "      s += ', num_layers={_num_layers}'\n",
        "    \n",
        "    if self._dir == 2:\n",
        "      s += ', bidirectional'\n",
        "    \n",
        "    s += ')'\n",
        "    shape = self.l0_i2h_weight_mu.shape\n",
        "    mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, \n",
        "                                  shape[0] // self._gates)\n",
        "    return s.format(name=self.__class__.__name__, mapping=mapping, \n",
        "                    **self.__dict__)\n",
        "    \n",
        "  def _collect_params_with_prefix(self, prefix=''):\n",
        "    if prefix:\n",
        "      prefix += '.'\n",
        "\n",
        "    pattern = re.compile(r'(l|r)(\\d)_(i2h|h2h)_(weight|bias)_(mu|rho)\\Z')\n",
        "    \n",
        "    def convert_key(m, bidirectional):\n",
        "      d, l, g, t, p = [m.group(i) for i in range(1, 6)]\n",
        "\n",
        "      if bidirectional:\n",
        "        return '_unfused.{}.{}_cell.{}_{}_{}'.format(l, d, g, t, p)\n",
        "      else:\n",
        "        return '_unfused.{}.{}_{}_{}'.format(l, g, t, p)\n",
        "    \n",
        "    bidirectional = any(pattern.match(k).group(1) == 'r' \n",
        "                        for k in self._reg_params)\n",
        "    ret = {prefix + convert_key(pattern.match(key), bidirectional) : val \n",
        "           for key, val in self._reg_params.items()}\n",
        "    \n",
        "    for name, child in self._children.items():\n",
        "      ret.update(child._collect_params_with_prefix(prefix + name))\n",
        "    \n",
        "    return ret\n",
        "\n",
        "  def state_info(self, batch_size=0):\n",
        "    return [{'shape': (self._num_layers * self._dir, batch_size, \n",
        "                       self._hidden_size),\n",
        "             '__layout__': 'LNC', 'dtype': self._dtype},\n",
        "            {'shape': (self._num_layers * self._dir, batch_size, \n",
        "                       self._hidden_size),\n",
        "             '__layout__': 'LNC', 'dtype': self._dtype}]\n",
        "\n",
        "  def cast(self, dtype):\n",
        "    super(BayesianLSTM, self).cast(dtype)\n",
        "    self._dtype = dtype\n",
        "\n",
        "  def begin_state(self, batch_size=0, func=nd.zeros, **kwargs):\n",
        "    states = []\n",
        "\n",
        "    for i, info in enumerate(self.state_info(batch_size)):\n",
        "\n",
        "      if info is not None:\n",
        "        info.update(kwargs)\n",
        "      else:\n",
        "        info = kwargs\n",
        "      \n",
        "      state = func(name='%sh0_%d' % (self.prefix, i), **info).as_np_ndarray()\n",
        "      states.append(state)\n",
        "    \n",
        "    return states\n",
        "  \n",
        "  def __call__(self, inputs, states=None, **kwargs):\n",
        "    self.skip_states = states is None\n",
        "    if states is None:\n",
        "      if isinstance(inputs, nd.NDArray):\n",
        "        batch_size = inputs.shape[1] # TNC layout\n",
        "        states = self.begin_state(batch_size, ctx=inputs.context, \n",
        "                                  dtype=inputs.dtype)\n",
        "      else:\n",
        "        states = self.begin_state(0, func=symbol.zeros)\n",
        "\n",
        "    if isinstance(states, gluon.tensor_types):\n",
        "      states = [states]\n",
        "\n",
        "    return super(BayesianLSTM, self).__call__(inputs, states, **kwargs)\n",
        "\n",
        "  def forward(self, x, *args):\n",
        "    out = super(BayesianLSTM, self).forward(x, *args)\n",
        "    # out = (outputs, states, total_kl_cost)\n",
        "    self._total_kl_cost = out[2]\n",
        "    return out[0] if self.skip_states else (out[0], out[1])\n",
        "  \n",
        "  def hybrid_forward(self, F, inputs, states, **kwargs):\n",
        "    if F is nd:\n",
        "      batch_size = inputs.shape[1] # TNC layout\n",
        "\n",
        "    if F is nd:\n",
        "      for state, info in zip(states, self.state_info(batch_size)):\n",
        "        if state.shape != info['shape']:\n",
        "          raise ValueError(\n",
        "            \"Invalid recurrent state shape. Expecting %s, got %s.\"%(\n",
        "                                      str(info['shape']), str(state.shape)))\n",
        "\n",
        "    return self._forward_kernel(F, inputs, states, **kwargs)\n",
        "\n",
        "  def _forward_kernel(self, F, inputs, states, **kwargs):\n",
        "    params_mu = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n",
        "                 for t in ['weight_mu', 'bias_mu']\n",
        "                 for l in range(self._num_layers)\n",
        "                 for d in ['l', 'r'][:self._dir]\n",
        "                 for g in ['i2h', 'h2h'])\n",
        "    params_rho = (kwargs['{}{}_{}_{}'.format(d, l, g, t)].reshape(-1)\n",
        "                  for t in ['weight_rho', 'bias_rho']\n",
        "                  for l in range(self._num_layers)\n",
        "                  for d in ['l', 'r'][:self._dir]\n",
        "                  for g in ['i2h', 'h2h'])\n",
        "    params_mu = F.np._internal.rnn_param_concat(*params_mu, dim=0)\n",
        "    params_rho = F.np._internal.rnn_param_concat(*params_rho, dim=0)\n",
        "    \n",
        "    if self._params_shape is None and F is nd:\n",
        "      self._params_shape = params_rho.shape\n",
        "\n",
        "    params_dist = CustomNormal(F, params_mu, params_rho, self._params_shape)\n",
        "\n",
        "    if autograd.is_training() or self._sample_mode:\n",
        "      params = params_dist.sample()\n",
        "    else:\n",
        "      params = params_dist.mu\n",
        "    \n",
        "    rnn_args = states\n",
        "\n",
        "    rnn = F.npx.rnn(inputs, params, *rnn_args, use_sequence_length=False,\n",
        "                    state_size=self._hidden_size, projection_size=None,\n",
        "                    num_layers=self._num_layers, bidirectional=self._dir == 2,\n",
        "                    p=0, state_outputs=True, mode='lstm',\n",
        "                    lstm_state_clip_min=None,\n",
        "                    lstm_state_clip_max=None,\n",
        "                    lstm_state_clip_nan=False)\n",
        "\n",
        "    outputs, states = rnn[0], [rnn[1], rnn[2]]\n",
        "    # We could save computation here.\n",
        "    total_kl_cost = self._get_total_kl_cost(F, params_dist, params)\n",
        "\n",
        "    return outputs, states, total_kl_cost\n",
        "\n",
        "  def kl_cost(self, scale=1.0):\n",
        "    assert self._total_kl_cost is not None, \\\n",
        "      'You must execute a forward operation before getting the KL cost'\n",
        "\n",
        "    return self._total_kl_cost * scale\n",
        "    \n",
        "  def _get_total_kl_cost(self, F, params_dist, params):\n",
        "    if F is nd:\n",
        "      ctx = self.l0_i2h_weight_mu.list_ctx()[0]\n",
        "      dtype = self.l0_i2h_weight_mu.dtype\n",
        "      prior = CustomScaleMixture(F, self._prior_pi, self._prior_sigma1, \n",
        "                                self._prior_sigma2, ctx, dtype)\n",
        "    else:\n",
        "      prior = CustomScaleMixture(F, self._prior_pi, self._prior_sigma1, \n",
        "                                self._prior_sigma2)\n",
        "\n",
        "    log_prior = prior.log_prob(params)\n",
        "    log_variational_posterior = params_dist.log_prob(params).sum()\n",
        "    return log_variational_posterior - log_prior\n",
        "  \n",
        "  @property\n",
        "  def sample_mode(self):\n",
        "    return self._sample_mode\n",
        "  \n",
        "  @sample_mode.setter\n",
        "  def sample_mode(self, value):\n",
        "    self._sample_mode = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUI_0tsA0MYA",
        "colab_type": "text"
      },
      "source": [
        "Por fim, vamos definir a classe `BayesianDense`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGSge-kajnQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianDense(nn.HybridBlock):\n",
        "\n",
        "  def __init__(self, units, in_units, prior_pi, prior_sigma1, prior_sigma2,\n",
        "               activation=None, use_bias=True, flatten=True, dtype='float32',\n",
        "               weight_mu_initializer=None, weight_rho_initializer=None, \n",
        "               bias_mu_initializer='zeros', bias_rho_initializer=None,\n",
        "               sample_mode=False, bbb_on_bias=True, **kwargs):\n",
        "    super(BayesianDense, self).__init__(**kwargs)\n",
        "    self._units = units\n",
        "    self._in_units = in_units\n",
        "    self._prior_pi = prior_pi\n",
        "    self._prior_sigma1 = prior_sigma1\n",
        "    self._prior_sigma2 = prior_sigma2\n",
        "    self._flatten = flatten\n",
        "    self._sample_mode = sample_mode\n",
        "    self._total_kl_cost = None\n",
        "    \n",
        "    with self.name_scope():\n",
        "      self.weight_mu = self.params.get('weight_mu',\n",
        "                                       shape=(units, in_units),\n",
        "                                       init=weight_mu_initializer,\n",
        "                                       dtype=dtype, allow_deferred_init=True)\n",
        "      self.weight_rho = self.params.get('weight_rho',\n",
        "                                        shape=(units, in_units),\n",
        "                                        init=weight_rho_initializer,\n",
        "                                        dtype=dtype, allow_deferred_init=True)\n",
        "      \n",
        "      if use_bias:\n",
        "        self.bias_mu = self.params.get('bias_mu', shape=(units,),\n",
        "                                       init=bias_mu_initializer,\n",
        "                                       dtype=dtype, allow_deferred_init=True)\n",
        "        \n",
        "        if bbb_on_bias:\n",
        "          self.bias_rho = self.params.get('bias_rho', shape=(units,),\n",
        "                                          init=bias_rho_initializer,\n",
        "                                          dtype=dtype, allow_deferred_init=True)\n",
        "        else:\n",
        "          self.bias_rho = None\n",
        "\n",
        "      else:\n",
        "        self.bias_mu = None\n",
        "        self.bias_rho = None\n",
        "      \n",
        "      if activation is not None:\n",
        "        self.act = nn.Activation(activation, prefix=activation+'_')\n",
        "      else:\n",
        "        self.act = None\n",
        "  \n",
        "  def __repr__(self):\n",
        "    s = '{name}({layout}, {act})'\n",
        "    shape = self.weight_mu.shape\n",
        "    return s.format(name=self.__class__.__name__,\n",
        "                    act=self.act if self.act else 'linear',\n",
        "                    layout='{0} -> {1}'.format(shape[1] if shape[1] else None, \n",
        "                                               shape[0]))\n",
        "\n",
        "  def forward(self, x, *args):\n",
        "    act, total_kl_cost = super(BayesianDense, self).forward(x, *args)\n",
        "    self._total_kl_cost = total_kl_cost\n",
        "    return act\n",
        "  \n",
        "  def hybrid_forward(self, F, x, weight_mu, weight_rho, \n",
        "                     bias_mu=None, bias_rho=None):\n",
        "    weight_dist = CustomNormal(F, weight_mu, weight_rho, self.weight_rho.shape)\n",
        "\n",
        "    if autograd.is_training() or self._sample_mode:\n",
        "      weight = weight_dist.sample()\n",
        "    else:\n",
        "      weight = weight_dist.mu\n",
        "\n",
        "    if bias_mu is not None:\n",
        "\n",
        "      if bias_rho is not None:\n",
        "        bias_dist = CustomNormal(F, bias_mu, bias_rho, self.bias_rho.shape)\n",
        "\n",
        "        if autograd.is_training() or self._sample_mode:\n",
        "          bias = bias_dist.sample()\n",
        "        else:\n",
        "          bias = bias_dist.mu\n",
        "\n",
        "      else:\n",
        "        bias = bias_mu\n",
        "\n",
        "    else:\n",
        "      bias = None\n",
        "\n",
        "    act = F.npx.fully_connected(x, weight, bias, no_bias=bias_mu is None, \n",
        "                                num_hidden=self._units, flatten=self._flatten, \n",
        "                                name='fwd')\n",
        "\n",
        "    if self.act is not None:\n",
        "      act = self.act(act)\n",
        "    \n",
        "    # We could save computation here.\n",
        "    if bias_rho is not None:\n",
        "      total_kl_cost = self._get_total_kl_cost(F, weight_dist, weight, \n",
        "                                              bias_dist, bias)\n",
        "    else:\n",
        "      total_kl_cost = self._get_total_kl_cost(F, weight_dist, weight)\n",
        "\n",
        "    return act, total_kl_cost\n",
        "  \n",
        "  def kl_cost(self, scale=1.0):\n",
        "    assert self._total_kl_cost is not None, \\\n",
        "      'You must execute a forward operation before getting the KL cost'\n",
        "\n",
        "    return self._total_kl_cost * scale\n",
        "\n",
        "  def _get_total_kl_cost(self, F, weight_dist, weight, \n",
        "                         bias_dist=None, bias=None):\n",
        "    if F is nd:\n",
        "      ctx = self.weight_mu.list_ctx()[0]\n",
        "      dtype = self.weight_mu.dtype\n",
        "      prior = CustomScaleMixture(F, self._prior_pi, self._prior_sigma1, \n",
        "                                self._prior_sigma2, ctx, dtype)\n",
        "    else:\n",
        "      prior = CustomScaleMixture(F, self._prior_pi, self._prior_sigma1, \n",
        "                                self._prior_sigma2)\n",
        "\n",
        "    if bias_dist is not None:\n",
        "      log_prior = prior.log_prob(weight) + prior.log_prob(bias)\n",
        "      log_variational_posterior = weight_dist.log_prob(weight).sum() + \\\n",
        "                                  bias_dist.log_prob(bias).sum()\n",
        "    else:\n",
        "      log_prior = prior.log_prob(weight)\n",
        "      log_variational_posterior = weight_dist.log_prob(weight).sum()\n",
        "    \n",
        "    return log_variational_posterior - log_prior\n",
        "  \n",
        "  @property\n",
        "  def sample_mode(self):\n",
        "    return self._sample_mode\n",
        "  \n",
        "  @sample_mode.setter\n",
        "  def sample_mode(self, value):\n",
        "    self._sample_mode = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyc4m1WSMWX8",
        "colab_type": "text"
      },
      "source": [
        "## **5. Modelo de Fortunato et al. [2017]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQm9R-0hcsXJ",
        "colab_type": "text"
      },
      "source": [
        "O modelo de Fortunato et al. [2017] se diferencia do anterior nos seguintes aspectos:\n",
        "\n",
        "*   Não há dropout.\n",
        "*   As camadas originais foram substituídas pelas suas respectivas versões bayesianas, definidas anteriormente.\n",
        "*   Esse modelo tem um método chamado `kl_cost` que retorna a soma do custo da complexidade de cada camada que compõe esse modelo.\n",
        "\n",
        "Seguindo o que foi proposto pelos autores, não atribuímos distribuição de probabilidade sobre o bias da camada densa, conforme valor corrente da variável `bbb_on_bias`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77f-ZXG6vjPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BayesianPTBModel(nn.HybridBlock):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_size=args.embedding_size,\n",
        "               hidden_size=args.hidden_size, num_layers=args.num_layers,\n",
        "               prior_pi=args.prior_pi, prior_sigma1=args.prior_sigma1, \n",
        "               prior_sigma2=args.prior_sigma2, tie_weights=args.tie_weights,\n",
        "               sample_mode=args.sample_mode, bbb_on_bias=args.bbb_on_bias,\n",
        "               **kwargs):\n",
        "    super(BayesianPTBModel, self).__init__(**kwargs)\n",
        "    self._sample_mode = sample_mode\n",
        "    self._total_kl_cost = None\n",
        "    non_lstm_rho_init = non_lstm_rho_initializer(prior_pi, prior_sigma1, \n",
        "                                                 prior_sigma2)\n",
        "    lstm_rho_init = lstm_rho_initializer(prior_pi, prior_sigma1, prior_sigma2)\n",
        "\n",
        "    with self.name_scope():\n",
        "      self.encoder = BayesianEmbedding(input_dim=vocab_size,\n",
        "                                       output_dim=embedding_size, \n",
        "                                       prior_pi=prior_pi,\n",
        "                                       prior_sigma1=prior_sigma1, \n",
        "                                       prior_sigma2=prior_sigma2,\n",
        "                                       weight_rho_initializer=non_lstm_rho_init,\n",
        "                                       sample_mode=sample_mode)\n",
        "      self.lstm = BayesianLSTM(input_size=embedding_size,\n",
        "                               hidden_size=hidden_size,\n",
        "                               prior_pi=prior_pi,\n",
        "                               prior_sigma1=prior_sigma1,\n",
        "                               prior_sigma2=prior_sigma2,\n",
        "                               num_layers=num_layers,\n",
        "                               i2h_weight_rho_initializer=lstm_rho_init,\n",
        "                               h2h_weight_rho_initializer=lstm_rho_init,\n",
        "                               i2h_bias_rho_initializer=lstm_rho_init,\n",
        "                               h2h_bias_rho_initializer=lstm_rho_init,\n",
        "                               sample_mode=sample_mode)\n",
        "      \n",
        "      if tie_weights:\n",
        "        self.decoder = BayesianDense(units=vocab_size,\n",
        "                                     in_units=hidden_size,\n",
        "                                     prior_pi=prior_pi,\n",
        "                                     prior_sigma1=prior_sigma1,\n",
        "                                     prior_sigma2=prior_sigma2,\n",
        "                                     weight_rho_initializer=non_lstm_rho_init,\n",
        "                                     bias_rho_initializer=non_lstm_rho_init,\n",
        "                                     sample_mode=sample_mode,\n",
        "                                     bbb_on_bias=bbb_on_bias,\n",
        "                                     params=self.encoder.params)\n",
        "      else:\n",
        "        self.decoder = BayesianDense(units=vocab_size,\n",
        "                                     in_units=hidden_size,\n",
        "                                     prior_pi=prior_pi,\n",
        "                                     prior_sigma1=prior_sigma1,\n",
        "                                     prior_sigma2=prior_sigma2,\n",
        "                                     weight_rho_initializer=non_lstm_rho_init,\n",
        "                                     bias_rho_initializer=non_lstm_rho_init,\n",
        "                                     sample_mode=sample_mode,\n",
        "                                     bbb_on_bias=bbb_on_bias)\n",
        "\n",
        "      self.hidden_size = hidden_size\n",
        "\n",
        "  def forward(self, x, *args):\n",
        "    out = super(BayesianPTBModel, self).forward(x, *args)\n",
        "    # out = (outputs, states, total_kl_cost)\n",
        "    self._total_kl_cost = out[2]\n",
        "    return out[0], out[1]\n",
        "\n",
        "  def hybrid_forward(self, F, inputs, state):\n",
        "    # inputs.shape = (batch_size, num_steps)\n",
        "    # encoded.shape = (num_steps, batch_size, embedding_size)\n",
        "    encoded = self.encoder(inputs.T)\n",
        "    # output.shape = (num_steps, batch_size, hidden_size)\n",
        "    # state[_].shape = (num_layers, batch_size, hidden_size)\n",
        "    output, state = self.lstm(encoded, state)\n",
        "    # decoded.shape = (num_steps * batch_size, vocab_size)\n",
        "    decoded = self.decoder(output.reshape((-1, self.hidden_size)))\n",
        "    total_kl_cost = self.encoder.kl_cost() + self.lstm.kl_cost() + \\\n",
        "                    self.decoder.kl_cost()\n",
        "    return decoded, state, total_kl_cost\n",
        "    \n",
        "  def begin_state(self, *args, **kwargs):\n",
        "    return self.lstm.begin_state(*args, **kwargs)\n",
        "  \n",
        "  def kl_cost(self, scale=1.0):\n",
        "    assert self._total_kl_cost is not None, \\\n",
        "      'You must execute a forward operation before getting the KL cost'\n",
        "\n",
        "    return self._total_kl_cost * scale\n",
        "  \n",
        "  @property\n",
        "  def sample_mode(self):\n",
        "    return self._sample_mode\n",
        "  \n",
        "  @sample_mode.setter\n",
        "  def sample_mode(self, value):\n",
        "    self._sample_mode = value\n",
        "    self.encoder.sample_mode = value\n",
        "    self.lstm.sample_mode = value\n",
        "    self.decoder.sample_mode = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFOix6Ssku39",
        "colab_type": "text"
      },
      "source": [
        "As configurações de treinamento desse modelo são ligeiramente diferentes das usadas no modelo anterior. Veja, abaixo, o que precisa ser alterado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EouBe_R6_uMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args.lr_decay = 0.9\n",
        "args.num_epochs = 70\n",
        "args.high_lr_epochs = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3jcIcVhmu0A",
        "colab_type": "text"
      },
      "source": [
        "No treinamento desse modelo, vamos estimar a divergência $\\text{KL}$ que queremos minimizar utilizando simulação Monte Carlo com apenas uma amostra gerada da aproximação variacional. E para validação e teste, os pesos utilizados na operação de *forward* são o vetor de médias dessa distribuição convergente. Fortunato et al. [2017] utilizaram esse mesmo procedimento para evitar maior custo computacional em comparação com o custo do modelo de Zaremba et al. [2014].\n",
        "\n",
        "Como o custo da complexidade da rede não depende dos dados, nós precisamos colocá-lo na mesma escala do custo da log-verossimilhança em cada mini-lote. De acordo com Blundell et al. [2015], existem várias formas de se fazer esse ajuste. A forma usada por Fortunato et al. [2017] pode ser considerada a mais simples e direta: basta dividir `total_kl_cost` por `batch_size * num_batches`. Desse modo, o custo da complexidade é distribuído uniformimente entre todas as subsequências de todos os mini-lotes do conjunto de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yoBiHn29MNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch_bbb(model, train_iter, loss, clip_norm, trainer, ctx):\n",
        "  start_time = time.time()\n",
        "  num_steps = train_iter.num_steps\n",
        "  batch_size = train_iter.batch_size\n",
        "  num_batches = train_iter.num_batches\n",
        "  num_dataset_elements = batch_size * num_batches\n",
        "  state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
        "  loss_sum = 0\n",
        "  steps_sum = 0\n",
        "  \n",
        "  for X, Y in train_iter:\n",
        "\n",
        "    for s in state: s.detach()\n",
        "    \n",
        "    X = X.as_in_context(ctx)\n",
        "    # Y.shape = (batch_size, num_steps)\n",
        "    y = Y.T.reshape((-1,))\n",
        "    # y.shape = (num_steps * batch_size)\n",
        "    y = y.as_in_context(ctx)\n",
        "\n",
        "    with autograd.record():\n",
        "      yhat, state = model(X, state)\n",
        "      L = loss(yhat, y)\n",
        "      # Sum over the sequence\n",
        "      sequence_neg_log_prob = L.reshape((batch_size, num_steps)).sum(axis=1)\n",
        "      # Average over the batch\n",
        "      data_loss = sequence_neg_log_prob.mean()\n",
        "      total_kl_cost = model.kl_cost()\n",
        "      scaled_kl_cost = total_kl_cost / num_dataset_elements\n",
        "      # KL divergence\n",
        "      total_loss = scaled_kl_cost + data_loss\n",
        "    \n",
        "    total_loss.backward()\n",
        "    grad_clipping(model, clip_norm)\n",
        "    trainer.step(batch_size=1)\n",
        "    loss_sum += data_loss\n",
        "    steps_sum += num_steps\n",
        "        \n",
        "  return loss_sum / steps_sum, time.time() - start_time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT8Z8lxv_NXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_bbb(model, train_iter, valid_iter, test_iter, \n",
        "              init_scale=args.init_scale, lr=args.lr_start, \n",
        "              lr_decay=args.lr_decay, num_epochs=args.num_epochs, \n",
        "              high_lr_epochs=args.high_lr_epochs, clip_norm=args.clip_norm, \n",
        "              ctx=context.cpu()):\n",
        "  loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "  model.initialize(ctx=ctx, force_reinit=True,\n",
        "                   init=init.Normal(sigma=init_scale))\n",
        "  trainer = gluon.Trainer(model.collect_params(), 'sgd', \n",
        "                          {'learning_rate': lr})\n",
        "    \n",
        "  # Train and check the progress\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    if epoch >= high_lr_epochs:\n",
        "      lr = lr * lr_decay\n",
        "      trainer._init_optimizer('sgd', {'learning_rate': lr})\n",
        "\n",
        "    train_loss, speed = train_epoch_bbb(model, train_iter, loss, clip_norm, \n",
        "                                        trainer, ctx)\n",
        "    print('[Epoch %d] time cost %.2fs, train loss %.2f, train ppl %.2f'%(\n",
        "          epoch, speed, train_loss, math.exp(train_loss)))\n",
        "    valid_loss = eval(model, valid_iter, loss, ctx)\n",
        "    print('valid loss %.2f, valid ppl %.2f'%(valid_loss, math.exp(valid_loss)))\n",
        "    test_loss = eval(model, test_iter, loss, ctx)\n",
        "    print('test loss %.2f, test ppl %.2f'%(test_loss, math.exp(test_loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlzSmbE_tafK",
        "colab_type": "text"
      },
      "source": [
        "Para finalizar, vamos instanciar esse modelo e executar o seu treinamento. A perplexidade por palavra reportada no artigo de Fortunato et al. [2017] foi de 78.8 no subconjunto de validação e de 75.5 no de teste.\n",
        "\n",
        "Uma vez que os pesos do modelo são inicializados de forma aleatória, os resultados podem variar cada vez que executamos o treinamento. O melhor resultado que obtivemos nesse notebook foi de 79.37 e 75.98 nos mesmos subconjuntos, nessa ordem. Executando mais vezes esse procedimento, provavelmente serão obtidos resultados melhores. Mas nós não fizemos mais tentativas por questão de tempo (e de custo)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bkmbQIiIDW7",
        "colab_type": "code",
        "outputId": "0a6f0b85-55ad-48ef-c0ac-bad0f115596d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BayesianPTBModel(len(vocab))\n",
        "train_bbb(model, train_iter, valid_iter, test_iter, ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0] time cost 62.14s, train loss 6.29, train ppl 538.67\n",
            "valid loss 5.65, valid ppl 284.94\n",
            "test loss 5.62, test ppl 277.05\n",
            "[Epoch 1] time cost 61.87s, train loss 5.43, train ppl 227.63\n",
            "valid loss 5.31, valid ppl 201.71\n",
            "test loss 5.28, test ppl 196.38\n",
            "[Epoch 2] time cost 62.25s, train loss 5.12, train ppl 168.16\n",
            "valid loss 5.07, valid ppl 159.37\n",
            "test loss 5.04, test ppl 155.23\n",
            "[Epoch 3] time cost 62.61s, train loss 4.95, train ppl 140.64\n",
            "valid loss 4.96, valid ppl 142.04\n",
            "test loss 4.93, test ppl 138.00\n",
            "[Epoch 4] time cost 62.45s, train loss 4.83, train ppl 124.66\n",
            "valid loss 4.87, valid ppl 130.52\n",
            "test loss 4.84, test ppl 127.08\n",
            "[Epoch 5] time cost 62.24s, train loss 4.73, train ppl 113.61\n",
            "valid loss 4.80, valid ppl 121.94\n",
            "test loss 4.77, test ppl 118.45\n",
            "[Epoch 6] time cost 62.75s, train loss 4.66, train ppl 105.86\n",
            "valid loss 4.76, valid ppl 117.33\n",
            "test loss 4.74, test ppl 114.60\n",
            "[Epoch 7] time cost 62.68s, train loss 4.61, train ppl 100.15\n",
            "valid loss 4.73, valid ppl 113.07\n",
            "test loss 4.70, test ppl 110.47\n",
            "[Epoch 8] time cost 62.57s, train loss 4.56, train ppl 95.60\n",
            "valid loss 4.70, valid ppl 110.23\n",
            "test loss 4.68, test ppl 107.52\n",
            "[Epoch 9] time cost 63.06s, train loss 4.52, train ppl 91.92\n",
            "valid loss 4.67, valid ppl 107.10\n",
            "test loss 4.65, test ppl 104.20\n",
            "[Epoch 10] time cost 62.31s, train loss 4.49, train ppl 89.17\n",
            "valid loss 4.64, valid ppl 104.02\n",
            "test loss 4.62, test ppl 101.35\n",
            "[Epoch 11] time cost 62.07s, train loss 4.46, train ppl 86.71\n",
            "valid loss 4.63, valid ppl 102.62\n",
            "test loss 4.61, test ppl 100.09\n",
            "[Epoch 12] time cost 62.51s, train loss 4.44, train ppl 84.82\n",
            "valid loss 4.63, valid ppl 102.48\n",
            "test loss 4.59, test ppl 98.95\n",
            "[Epoch 13] time cost 62.47s, train loss 4.42, train ppl 83.20\n",
            "valid loss 4.61, valid ppl 100.49\n",
            "test loss 4.58, test ppl 97.74\n",
            "[Epoch 14] time cost 62.64s, train loss 4.41, train ppl 82.04\n",
            "valid loss 4.60, valid ppl 99.64\n",
            "test loss 4.57, test ppl 96.56\n",
            "[Epoch 15] time cost 62.30s, train loss 4.39, train ppl 80.90\n",
            "valid loss 4.59, valid ppl 98.26\n",
            "test loss 4.56, test ppl 95.49\n",
            "[Epoch 16] time cost 62.94s, train loss 4.38, train ppl 80.17\n",
            "valid loss 4.58, valid ppl 97.42\n",
            "test loss 4.55, test ppl 94.55\n",
            "[Epoch 17] time cost 62.19s, train loss 4.37, train ppl 79.41\n",
            "valid loss 4.58, valid ppl 97.04\n",
            "test loss 4.54, test ppl 93.74\n",
            "[Epoch 18] time cost 61.95s, train loss 4.37, train ppl 78.84\n",
            "valid loss 4.56, valid ppl 95.37\n",
            "test loss 4.53, test ppl 92.36\n",
            "[Epoch 19] time cost 62.77s, train loss 4.36, train ppl 78.54\n",
            "valid loss 4.56, valid ppl 95.83\n",
            "test loss 4.53, test ppl 92.56\n",
            "[Epoch 20] time cost 62.28s, train loss 4.34, train ppl 76.60\n",
            "valid loss 4.55, valid ppl 94.24\n",
            "test loss 4.51, test ppl 91.15\n",
            "[Epoch 21] time cost 63.12s, train loss 4.32, train ppl 74.97\n",
            "valid loss 4.52, valid ppl 91.62\n",
            "test loss 4.49, test ppl 89.04\n",
            "[Epoch 22] time cost 62.39s, train loss 4.29, train ppl 73.25\n",
            "valid loss 4.51, valid ppl 90.56\n",
            "test loss 4.47, test ppl 87.78\n",
            "[Epoch 23] time cost 62.08s, train loss 4.27, train ppl 71.87\n",
            "valid loss 4.49, valid ppl 89.18\n",
            "test loss 4.46, test ppl 86.78\n",
            "[Epoch 24] time cost 62.31s, train loss 4.26, train ppl 70.55\n",
            "valid loss 4.48, valid ppl 88.40\n",
            "test loss 4.45, test ppl 85.38\n",
            "[Epoch 25] time cost 62.54s, train loss 4.24, train ppl 69.63\n",
            "valid loss 4.47, valid ppl 87.03\n",
            "test loss 4.44, test ppl 84.49\n",
            "[Epoch 26] time cost 62.76s, train loss 4.23, train ppl 68.57\n",
            "valid loss 4.46, valid ppl 86.32\n",
            "test loss 4.43, test ppl 83.52\n",
            "[Epoch 27] time cost 61.95s, train loss 4.22, train ppl 67.70\n",
            "valid loss 4.45, valid ppl 85.46\n",
            "test loss 4.41, test ppl 82.67\n",
            "[Epoch 28] time cost 62.06s, train loss 4.20, train ppl 66.86\n",
            "valid loss 4.44, valid ppl 85.04\n",
            "test loss 4.41, test ppl 81.98\n",
            "[Epoch 29] time cost 62.37s, train loss 4.19, train ppl 66.10\n",
            "valid loss 4.44, valid ppl 84.64\n",
            "test loss 4.40, test ppl 81.83\n",
            "[Epoch 30] time cost 62.59s, train loss 4.18, train ppl 65.56\n",
            "valid loss 4.43, valid ppl 83.82\n",
            "test loss 4.40, test ppl 81.16\n",
            "[Epoch 31] time cost 62.58s, train loss 4.17, train ppl 64.84\n",
            "valid loss 4.42, valid ppl 83.44\n",
            "test loss 4.39, test ppl 80.66\n",
            "[Epoch 32] time cost 62.26s, train loss 4.17, train ppl 64.43\n",
            "valid loss 4.42, valid ppl 83.03\n",
            "test loss 4.38, test ppl 80.08\n",
            "[Epoch 33] time cost 62.72s, train loss 4.16, train ppl 63.93\n",
            "valid loss 4.41, valid ppl 82.64\n",
            "test loss 4.38, test ppl 79.81\n",
            "[Epoch 34] time cost 62.31s, train loss 4.15, train ppl 63.55\n",
            "valid loss 4.41, valid ppl 82.36\n",
            "test loss 4.37, test ppl 79.35\n",
            "[Epoch 35] time cost 62.73s, train loss 4.15, train ppl 63.13\n",
            "valid loss 4.41, valid ppl 82.14\n",
            "test loss 4.37, test ppl 79.22\n",
            "[Epoch 36] time cost 62.23s, train loss 4.14, train ppl 62.75\n",
            "valid loss 4.40, valid ppl 81.62\n",
            "test loss 4.37, test ppl 78.79\n",
            "[Epoch 37] time cost 62.27s, train loss 4.13, train ppl 62.39\n",
            "valid loss 4.40, valid ppl 81.51\n",
            "test loss 4.37, test ppl 78.66\n",
            "[Epoch 38] time cost 62.44s, train loss 4.13, train ppl 62.20\n",
            "valid loss 4.40, valid ppl 81.20\n",
            "test loss 4.36, test ppl 78.21\n",
            "[Epoch 39] time cost 62.66s, train loss 4.13, train ppl 61.99\n",
            "valid loss 4.40, valid ppl 81.14\n",
            "test loss 4.36, test ppl 78.17\n",
            "[Epoch 40] time cost 63.19s, train loss 4.12, train ppl 61.73\n",
            "valid loss 4.39, valid ppl 81.01\n",
            "test loss 4.36, test ppl 77.96\n",
            "[Epoch 41] time cost 62.34s, train loss 4.12, train ppl 61.50\n",
            "valid loss 4.39, valid ppl 80.90\n",
            "test loss 4.36, test ppl 77.91\n",
            "[Epoch 42] time cost 62.57s, train loss 4.12, train ppl 61.35\n",
            "valid loss 4.39, valid ppl 80.80\n",
            "test loss 4.35, test ppl 77.77\n",
            "[Epoch 43] time cost 62.48s, train loss 4.12, train ppl 61.29\n",
            "valid loss 4.39, valid ppl 80.65\n",
            "test loss 4.35, test ppl 77.58\n",
            "[Epoch 44] time cost 62.50s, train loss 4.11, train ppl 61.08\n",
            "valid loss 4.39, valid ppl 80.58\n",
            "test loss 4.35, test ppl 77.54\n",
            "[Epoch 45] time cost 62.82s, train loss 4.11, train ppl 60.84\n",
            "valid loss 4.39, valid ppl 80.46\n",
            "test loss 4.35, test ppl 77.39\n",
            "[Epoch 46] time cost 62.38s, train loss 4.11, train ppl 60.71\n",
            "valid loss 4.39, valid ppl 80.35\n",
            "test loss 4.35, test ppl 77.31\n",
            "[Epoch 47] time cost 62.25s, train loss 4.10, train ppl 60.64\n",
            "valid loss 4.39, valid ppl 80.36\n",
            "test loss 4.35, test ppl 77.30\n",
            "[Epoch 48] time cost 62.07s, train loss 4.10, train ppl 60.64\n",
            "valid loss 4.38, valid ppl 80.17\n",
            "test loss 4.35, test ppl 77.09\n",
            "[Epoch 49] time cost 62.57s, train loss 4.10, train ppl 60.51\n",
            "valid loss 4.38, valid ppl 80.10\n",
            "test loss 4.34, test ppl 77.06\n",
            "[Epoch 50] time cost 62.67s, train loss 4.10, train ppl 60.38\n",
            "valid loss 4.38, valid ppl 80.11\n",
            "test loss 4.34, test ppl 76.99\n",
            "[Epoch 51] time cost 62.46s, train loss 4.10, train ppl 60.34\n",
            "valid loss 4.38, valid ppl 80.03\n",
            "test loss 4.34, test ppl 76.94\n",
            "[Epoch 52] time cost 62.23s, train loss 4.10, train ppl 60.19\n",
            "valid loss 4.38, valid ppl 80.01\n",
            "test loss 4.34, test ppl 76.90\n",
            "[Epoch 53] time cost 62.24s, train loss 4.10, train ppl 60.12\n",
            "valid loss 4.38, valid ppl 80.01\n",
            "test loss 4.34, test ppl 76.85\n",
            "[Epoch 54] time cost 62.36s, train loss 4.10, train ppl 60.18\n",
            "valid loss 4.38, valid ppl 79.91\n",
            "test loss 4.34, test ppl 76.76\n",
            "[Epoch 55] time cost 62.26s, train loss 4.10, train ppl 60.06\n",
            "valid loss 4.38, valid ppl 79.85\n",
            "test loss 4.34, test ppl 76.68\n",
            "[Epoch 56] time cost 62.21s, train loss 4.09, train ppl 60.03\n",
            "valid loss 4.38, valid ppl 79.85\n",
            "test loss 4.34, test ppl 76.69\n",
            "[Epoch 57] time cost 62.35s, train loss 4.09, train ppl 59.96\n",
            "valid loss 4.38, valid ppl 79.85\n",
            "test loss 4.34, test ppl 76.70\n",
            "[Epoch 58] time cost 62.49s, train loss 4.09, train ppl 59.98\n",
            "valid loss 4.38, valid ppl 79.83\n",
            "test loss 4.34, test ppl 76.65\n",
            "[Epoch 59] time cost 62.92s, train loss 4.09, train ppl 59.82\n",
            "valid loss 4.38, valid ppl 79.80\n",
            "test loss 4.34, test ppl 76.62\n",
            "[Epoch 60] time cost 62.32s, train loss 4.09, train ppl 59.87\n",
            "valid loss 4.38, valid ppl 79.76\n",
            "test loss 4.34, test ppl 76.60\n",
            "[Epoch 61] time cost 62.24s, train loss 4.09, train ppl 59.88\n",
            "valid loss 4.38, valid ppl 79.76\n",
            "test loss 4.34, test ppl 76.60\n",
            "[Epoch 62] time cost 61.99s, train loss 4.09, train ppl 59.82\n",
            "valid loss 4.38, valid ppl 79.73\n",
            "test loss 4.34, test ppl 76.56\n",
            "[Epoch 63] time cost 62.71s, train loss 4.09, train ppl 59.75\n",
            "valid loss 4.38, valid ppl 79.67\n",
            "test loss 4.34, test ppl 76.50\n",
            "[Epoch 64] time cost 62.45s, train loss 4.09, train ppl 59.80\n",
            "valid loss 4.38, valid ppl 79.65\n",
            "test loss 4.34, test ppl 76.47\n",
            "[Epoch 65] time cost 62.77s, train loss 4.09, train ppl 59.81\n",
            "valid loss 4.38, valid ppl 79.67\n",
            "test loss 4.34, test ppl 76.51\n",
            "[Epoch 66] time cost 62.60s, train loss 4.09, train ppl 59.74\n",
            "valid loss 4.38, valid ppl 79.67\n",
            "test loss 4.34, test ppl 76.48\n",
            "[Epoch 67] time cost 62.15s, train loss 4.09, train ppl 59.65\n",
            "valid loss 4.38, valid ppl 79.67\n",
            "test loss 4.34, test ppl 76.47\n",
            "[Epoch 68] time cost 62.38s, train loss 4.09, train ppl 59.75\n",
            "valid loss 4.38, valid ppl 79.67\n",
            "test loss 4.34, test ppl 76.48\n",
            "[Epoch 69] time cost 62.61s, train loss 4.09, train ppl 59.64\n",
            "valid loss 4.38, valid ppl 79.66\n",
            "test loss 4.34, test ppl 76.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hub1P9OoyQZu",
        "colab_type": "text"
      },
      "source": [
        "## **6. Considerações finais**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHC0H8CZR9hh",
        "colab_type": "text"
      },
      "source": [
        "Segue, abaixo, uma lista dos próximos passos desse trabalho. A existência dessa lista, entretanto, não implica que os objetivos desse TP não foram alcançados. Com efeito, conseguimos alcançar resultados similares aos de Zaremba et al. [2014] e Fortunato et al. [2017] nas configurações de interesse. Mas com o conhecimento que foi adquirido ao longo desse trabalho, agora é possível fazer mais e melhor.\n",
        "\n",
        "*   Suportar simulação Monte Carlo no modo de inferência da rede neural bayesiana. Na presente versão desse TP, os pesos da rede que são usados durante a inferência são o vetor de médias da aproximação variacional.\n",
        "*   Suportar `num_samples` amostras na simulação Monte Carlo dos modos de treinamento e inferência da rede neural bayesiana. Na atual versão desse TP, usamos apenas uma amostra por operação de *forward* durante o treinamento da rede.\n",
        "*   Concluir suporte a programação simbólica na rede neural bayesiana. Desse modo, será possível compilar o grafo computacional da operação de *forward* dessa rede e melhorar a sua eficiência computacional, tanto em termos de tempo de execução quanto de uso de memória. Atualmente, se você chamar o método `hybridize` do modelo bayesiano, não será possível treinar corretamente a rede.\n",
        "*   Tornar a interface e o código da rede neural bayesiana mais simples, organizado, confiável, manutenível e expansível. Com o conhecimento adquirido sobre a abordagem bayesiana para redes neurais e sobre o funcionamento e a organização do Apache MXNet, acredito ser possível desenvolver uma forma simples de combinar essa abordagem com muitos modelos de aprendizado profundo.\n",
        "*   Explorar utilização da informação sobre incerteza que é retornada pela rede neural bayesiana. Por exemplo, poda de parâmetros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQY1H7nHVX8",
        "colab_type": "text"
      },
      "source": [
        "## **7. Referências bibliográficas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2Dza_E_HfVN",
        "colab_type": "text"
      },
      "source": [
        "BLUNDELL, Charles et al. Weight uncertainty in neural networks. **arXiv preprint arXiv:1505.05424**, 2015.\n",
        "\n",
        "FORTUNATO, Meire; BLUNDELL, Charles; VINYALS, Oriol. Bayesian recurrent neural networks. **arXiv preprint arXiv:1704.02798**, 2017.\n",
        "\n",
        "MARCUS, Mitchell; SANTORINI, Beatrice; MARCINKIEWICZ, Mary Ann. Building a large annotated corpus of English: The Penn Treebank. 1993.\n",
        "\n",
        "MIKOLOV, Tomáš et al. Recurrent neural network based language model. In: **Eleventh annual conference of the international speech communication association**. 2010.\n",
        "\n",
        "ZAREMBA, Wojciech; SUTSKEVER, Ilya; VINYALS, Oriol. Recurrent neural network regularization. **arXiv preprint arXiv:1409.2329**, 2014.\n",
        "\n",
        "ZHANG, Aston et al. Dive into Deep Learning. 2019."
      ]
    }
  ]
}